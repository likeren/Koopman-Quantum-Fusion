{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b3b7d0-2709-4195-9f35-7e49a2c567fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 .npy 数据 (可能需要几秒钟)...\n",
      "加载成功。样本数: 4763\n",
      "----------------------------------------\n",
      "模型结构: KoopmanCNN\n",
      "总参数量 (Total params): 9,250\n",
      "可训练参数量 (Trainable params): 9,250\n",
      "----------------------------------------\n",
      "开始在 cpu 上训练 DeepRawCNN...\n",
      "Epoch [1/30] | Loss: 0.3436 | Train Acc: 92.95% | Test Acc: 93.42%\n",
      "Epoch [2/30] | Loss: 0.2080 | Train Acc: 94.81% | Test Acc: 93.28%\n",
      "Epoch [3/30] | Loss: 0.1803 | Train Acc: 94.90% | Test Acc: 93.42%\n",
      "Epoch [4/30] | Loss: 0.1612 | Train Acc: 95.05% | Test Acc: 93.63%\n",
      "Epoch [5/30] | Loss: 0.1537 | Train Acc: 95.53% | Test Acc: 93.77%\n",
      "Epoch [6/30] | Loss: 0.1448 | Train Acc: 95.68% | Test Acc: 93.77%\n",
      "Epoch [7/30] | Loss: 0.1421 | Train Acc: 95.86% | Test Acc: 93.63%\n",
      "Epoch [8/30] | Loss: 0.1329 | Train Acc: 95.98% | Test Acc: 93.84%\n",
      "Epoch [9/30] | Loss: 0.1343 | Train Acc: 96.01% | Test Acc: 93.98%\n",
      "Epoch [10/30] | Loss: 0.1301 | Train Acc: 96.16% | Test Acc: 94.33%\n",
      "Epoch [11/30] | Loss: 0.1269 | Train Acc: 96.19% | Test Acc: 94.26%\n",
      "Epoch [12/30] | Loss: 0.1245 | Train Acc: 96.25% | Test Acc: 94.40%\n",
      "Epoch [13/30] | Loss: 0.1261 | Train Acc: 96.34% | Test Acc: 94.47%\n",
      "Epoch [14/30] | Loss: 0.1213 | Train Acc: 96.43% | Test Acc: 94.96%\n",
      "Epoch [15/30] | Loss: 0.1189 | Train Acc: 96.61% | Test Acc: 94.33%\n",
      "Epoch [16/30] | Loss: 0.1179 | Train Acc: 96.55% | Test Acc: 94.75%\n",
      "Epoch [17/30] | Loss: 0.1163 | Train Acc: 96.52% | Test Acc: 94.47%\n",
      "Epoch [18/30] | Loss: 0.1152 | Train Acc: 96.67% | Test Acc: 94.89%\n",
      "Epoch [19/30] | Loss: 0.1144 | Train Acc: 96.64% | Test Acc: 94.68%\n",
      "Epoch [20/30] | Loss: 0.1127 | Train Acc: 96.70% | Test Acc: 94.61%\n",
      "Epoch [21/30] | Loss: 0.1087 | Train Acc: 96.94% | Test Acc: 94.96%\n",
      "Epoch [22/30] | Loss: 0.1118 | Train Acc: 96.58% | Test Acc: 94.75%\n",
      "Epoch [23/30] | Loss: 0.1146 | Train Acc: 96.73% | Test Acc: 94.75%\n",
      "Epoch [24/30] | Loss: 0.1071 | Train Acc: 96.82% | Test Acc: 94.68%\n",
      "Epoch [25/30] | Loss: 0.1097 | Train Acc: 96.73% | Test Acc: 95.24%\n",
      "Epoch [26/30] | Loss: 0.1063 | Train Acc: 96.82% | Test Acc: 94.68%\n",
      "Epoch [27/30] | Loss: 0.1069 | Train Acc: 96.76% | Test Acc: 95.31%\n",
      "Epoch [28/30] | Loss: 0.1083 | Train Acc: 96.76% | Test Acc: 94.75%\n",
      "Epoch [29/30] | Loss: 0.1060 | Train Acc: 96.91% | Test Acc: 94.75%\n",
      "Epoch [30/30] | Loss: 0.1077 | Train Acc: 96.97% | Test Acc: 94.96%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "DATA_FILE = r'./data/raw_dataset_standardized.npy'\n",
    "BATCH_SIZE = 16       # 原始数据显存占用大，Batch Size 调小一点\n",
    "EPOCHS = 30           # 原始数据收敛慢，Epoch 调大\n",
    "LEARNING_RATE = 1e-4  # 学习率调小一点防止震荡\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =======================================\n",
    "\n",
    "class RawDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 转换为 Tensor\n",
    "        return torch.from_numpy(self.sequences[idx]), self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"处理变长序列，用 0 填充\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    # batch_first=True -> (Batch, Max_Len)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # 增加 Channel 维度 -> (Batch, Max_Len, 1)\n",
    "    padded_seqs = padded_seqs.unsqueeze(-1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_seqs, labels\n",
    "\n",
    "# --- Deep 1D-CNN 模型 (针对长序列) ---\n",
    "class DeepRawCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeepRawCNN, self).__init__()\n",
    "        # 输入: (Batch, 1, Length)\n",
    "        # Layer 1: 大感受野，快速降维\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=11, stride=4, padding=5), # Stride=4 长度直接除以4\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2) # 长度再除以2 (总共除以8)\n",
    "        )\n",
    "        \n",
    "        # Layer 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2) \n",
    "        )\n",
    "        \n",
    "        # Layer 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Layer 4 (Deep)\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 全局池化，无论多长都变成 (Batch, 128, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # 防止过拟合\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Length, 1) -> permute to (Batch, 1, Length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "class KoopmanCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=2):\n",
    "        super(KoopmanCNN, self).__init__()\n",
    "        \n",
    "        # 输入形状: (Batch, Channel=1, Length)\n",
    "        # 注意：在 forward 里我们需要把数据转置一下\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 全局平均池化，不管长度多少，最后都变成1\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x shape: (Batch, Length, 1)\n",
    "        # Conv1d 需要: (Batch, Channel, Length)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out) # -> (Batch, 64, 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # -> (Batch, 64)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train():\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"错误：找不到数据文件 {DATA_FILE}，请先运行数据生成脚本。\")\n",
    "        return\n",
    "\n",
    "    # 1. 加载数据\n",
    "    print(\"正在加载 .npy 数据 (可能需要几秒钟)...\")\n",
    "    data_dict = np.load(DATA_FILE, allow_pickle=True).item()\n",
    "    sequences = data_dict['sequences']\n",
    "    labels = data_dict['labels']\n",
    "    \n",
    "    print(f\"加载成功。样本数: {len(labels)}\")\n",
    "    \n",
    "    # 2. 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(RawDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(RawDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 3. 初始化\n",
    "    model = KoopmanCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    # ==========================================\n",
    "    # 【新增】输出参数量统计\n",
    "    # ==========================================\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"模型结构: {model.__class__.__name__}\")\n",
    "    print(f\"总参数量 (Total params): {total_params:,}\")\n",
    "    print(f\"可训练参数量 (Trainable params): {trainable_params:,}\")\n",
    "    print(\"-\" * 40)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"开始在 {DEVICE} 上训练 DeepRawCNN...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += targets.size(0)\n",
    "                test_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6c38b7-80a0-481e-beb6-9750f7957167",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 .npy 数据 (可能需要几秒钟)...\n",
      "加载成功。样本数: 4763\n",
      "----------------------------------------\n",
      "模型结构: DeepRawCNN\n",
      "总参数量 (Total params): 46,658\n",
      "可训练参数量 (Trainable params): 46,658\n",
      "----------------------------------------\n",
      "开始在 cpu 上训练 DeepRawCNN...\n",
      "Epoch [1/50] | Loss: 0.3698 | Train Acc: 90.49% | Test Acc: 93.42%\n",
      "Epoch [2/50] | Loss: 0.1707 | Train Acc: 95.05% | Test Acc: 93.91%\n",
      "Epoch [3/50] | Loss: 0.1478 | Train Acc: 95.59% | Test Acc: 93.91%\n",
      "Epoch [4/50] | Loss: 0.1286 | Train Acc: 96.58% | Test Acc: 94.75%\n",
      "Epoch [5/50] | Loss: 0.1270 | Train Acc: 96.52% | Test Acc: 95.31%\n",
      "Epoch [6/50] | Loss: 0.1154 | Train Acc: 96.67% | Test Acc: 95.59%\n",
      "Epoch [7/50] | Loss: 0.1155 | Train Acc: 96.76% | Test Acc: 95.94%\n",
      "Epoch [8/50] | Loss: 0.1131 | Train Acc: 97.12% | Test Acc: 96.01%\n",
      "Epoch [9/50] | Loss: 0.1026 | Train Acc: 97.06% | Test Acc: 95.52%\n",
      "Epoch [10/50] | Loss: 0.1030 | Train Acc: 97.42% | Test Acc: 96.22%\n",
      "Epoch [11/50] | Loss: 0.0982 | Train Acc: 97.30% | Test Acc: 96.08%\n",
      "Epoch [12/50] | Loss: 0.0952 | Train Acc: 97.57% | Test Acc: 95.66%\n",
      "Epoch [13/50] | Loss: 0.0893 | Train Acc: 97.78% | Test Acc: 96.64%\n",
      "Epoch [14/50] | Loss: 0.0885 | Train Acc: 97.75% | Test Acc: 96.15%\n",
      "Epoch [15/50] | Loss: 0.0935 | Train Acc: 97.54% | Test Acc: 96.43%\n",
      "Epoch [16/50] | Loss: 0.0881 | Train Acc: 97.51% | Test Acc: 96.29%\n",
      "Epoch [17/50] | Loss: 0.0904 | Train Acc: 97.72% | Test Acc: 96.43%\n",
      "Epoch [18/50] | Loss: 0.0911 | Train Acc: 97.60% | Test Acc: 96.57%\n",
      "Epoch [19/50] | Loss: 0.0849 | Train Acc: 97.81% | Test Acc: 97.13%\n",
      "Epoch [20/50] | Loss: 0.0803 | Train Acc: 97.81% | Test Acc: 97.06%\n",
      "Epoch [21/50] | Loss: 0.0796 | Train Acc: 97.81% | Test Acc: 96.64%\n",
      "Epoch [22/50] | Loss: 0.0789 | Train Acc: 98.02% | Test Acc: 96.78%\n",
      "Epoch [23/50] | Loss: 0.0745 | Train Acc: 97.90% | Test Acc: 96.85%\n",
      "Epoch [24/50] | Loss: 0.0747 | Train Acc: 97.96% | Test Acc: 97.20%\n",
      "Epoch [25/50] | Loss: 0.0728 | Train Acc: 97.93% | Test Acc: 97.27%\n",
      "Epoch [26/50] | Loss: 0.0723 | Train Acc: 97.99% | Test Acc: 96.64%\n",
      "Epoch [27/50] | Loss: 0.0725 | Train Acc: 98.29% | Test Acc: 97.06%\n",
      "Epoch [28/50] | Loss: 0.0713 | Train Acc: 97.93% | Test Acc: 97.27%\n",
      "Epoch [29/50] | Loss: 0.0664 | Train Acc: 98.14% | Test Acc: 96.99%\n",
      "Epoch [30/50] | Loss: 0.0661 | Train Acc: 98.05% | Test Acc: 97.06%\n",
      "Epoch [31/50] | Loss: 0.0712 | Train Acc: 98.05% | Test Acc: 97.20%\n",
      "Epoch [32/50] | Loss: 0.0656 | Train Acc: 98.20% | Test Acc: 97.20%\n",
      "Epoch [33/50] | Loss: 0.0639 | Train Acc: 98.35% | Test Acc: 97.13%\n",
      "Epoch [34/50] | Loss: 0.0644 | Train Acc: 98.20% | Test Acc: 97.69%\n",
      "Epoch [35/50] | Loss: 0.0663 | Train Acc: 98.08% | Test Acc: 97.48%\n",
      "Epoch [36/50] | Loss: 0.0660 | Train Acc: 98.35% | Test Acc: 96.92%\n",
      "Epoch [37/50] | Loss: 0.0557 | Train Acc: 98.56% | Test Acc: 97.20%\n",
      "Epoch [38/50] | Loss: 0.0584 | Train Acc: 98.47% | Test Acc: 97.20%\n",
      "Epoch [39/50] | Loss: 0.0616 | Train Acc: 98.23% | Test Acc: 97.27%\n",
      "Epoch [40/50] | Loss: 0.0520 | Train Acc: 98.68% | Test Acc: 97.20%\n",
      "Epoch [41/50] | Loss: 0.0623 | Train Acc: 98.20% | Test Acc: 98.04%\n",
      "Epoch [42/50] | Loss: 0.0554 | Train Acc: 98.44% | Test Acc: 97.06%\n",
      "Epoch [43/50] | Loss: 0.0560 | Train Acc: 98.47% | Test Acc: 97.34%\n",
      "Epoch [44/50] | Loss: 0.0567 | Train Acc: 98.47% | Test Acc: 96.92%\n",
      "Epoch [45/50] | Loss: 0.0607 | Train Acc: 98.41% | Test Acc: 97.97%\n",
      "Epoch [46/50] | Loss: 0.0552 | Train Acc: 98.62% | Test Acc: 96.64%\n",
      "Epoch [47/50] | Loss: 0.0507 | Train Acc: 98.65% | Test Acc: 97.06%\n",
      "Epoch [48/50] | Loss: 0.0467 | Train Acc: 98.77% | Test Acc: 96.99%\n",
      "Epoch [49/50] | Loss: 0.0511 | Train Acc: 98.62% | Test Acc: 97.62%\n",
      "Epoch [50/50] | Loss: 0.0431 | Train Acc: 98.68% | Test Acc: 97.27%\n",
      "\n",
      ">>> 训练数据已保存至: training_log_cnn_raw.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "DATA_FILE = r'./data/raw_dataset_standardized.npy'\n",
    "BATCH_SIZE = 16       # 原始数据显存占用大，Batch Size 调小一点\n",
    "EPOCHS = 50           # 原始数据收敛慢，Epoch 调大\n",
    "LEARNING_RATE = 1e-4  # 学习率调小一点防止震荡\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =======================================\n",
    "\n",
    "class RawDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 转换为 Tensor\n",
    "        return torch.from_numpy(self.sequences[idx]), self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"处理变长序列，用 0 填充\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    # batch_first=True -> (Batch, Max_Len)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # 增加 Channel 维度 -> (Batch, Max_Len, 1)\n",
    "    padded_seqs = padded_seqs.unsqueeze(-1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_seqs, labels\n",
    "\n",
    "# --- Deep 1D-CNN 模型 (针对长序列) ---\n",
    "class DeepRawCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeepRawCNN, self).__init__()\n",
    "        # 输入: (Batch, 1, Length)\n",
    "        # Layer 1: 大感受野，快速降维\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=11, stride=4, padding=5), # Stride=4 长度直接除以4\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2) # 长度再除以2 (总共除以8)\n",
    "        )\n",
    "        \n",
    "        # Layer 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2) \n",
    "        )\n",
    "        \n",
    "        # Layer 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Layer 4 (Deep)\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 全局池化，无论多长都变成 (Batch, 128, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # 防止过拟合\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Length, 1) -> permute to (Batch, 1, Length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "class KoopmanCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=2):\n",
    "        super(KoopmanCNN, self).__init__()\n",
    "        \n",
    "        # 输入形状: (Batch, Channel=1, Length)\n",
    "        # 注意：在 forward 里我们需要把数据转置一下\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 全局平均池化，不管长度多少，最后都变成1\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x shape: (Batch, Length, 1)\n",
    "        # Conv1d 需要: (Batch, Channel, Length)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out) # -> (Batch, 64, 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # -> (Batch, 64)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train():\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"错误：找不到数据文件 {DATA_FILE}，请先运行数据生成脚本。\")\n",
    "        return\n",
    "\n",
    "    # 1. 加载数据\n",
    "    print(\"正在加载 .npy 数据 (可能需要几秒钟)...\")\n",
    "    data_dict = np.load(DATA_FILE, allow_pickle=True).item()\n",
    "    sequences = data_dict['sequences']\n",
    "    labels = data_dict['labels']\n",
    "    \n",
    "    print(f\"加载成功。样本数: {len(labels)}\")\n",
    "    \n",
    "    # 2. 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(RawDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(RawDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 3. 初始化\n",
    "    model = DeepRawCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    # ==========================================\n",
    "    # 【新增】输出参数量统计\n",
    "    # ==========================================\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"模型结构: {model.__class__.__name__}\")\n",
    "    print(f\"总参数量 (Total params): {total_params:,}\")\n",
    "    print(f\"可训练参数量 (Trainable params): {trainable_params:,}\")\n",
    "    print(\"-\" * 40)\n",
    "    # ==========================================\n",
    "    \n",
    "# ==========================================\n",
    "    # 【新增 1】 初始化记录列表\n",
    "    # ==========================================\n",
    "    history = {\n",
    "        'epochs': [],\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'params': total_params  # 顺便把参数量也存下来\n",
    "    }\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"开始在 {DEVICE} 上训练 DeepRawCNN...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader) # 计算平均 Loss\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += targets.size(0)\n",
    "                test_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        # ==========================================\n",
    "        # 【新增 2】 记录当前 Epoch 的数据\n",
    "        # ==========================================\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        # ==========================================\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 【新增 3】 训练结束后保存数据\n",
    "    # ==========================================\n",
    "    save_filename = 'training_log_cnn_raw.npy'\n",
    "    np.save(save_filename, history)\n",
    "    print(f\"\\n>>> 训练数据已保存至: {save_filename}\")\n",
    "    # ==========================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # 服务器端绘图\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "CONFIG = {\n",
    "    'data_file': r'./data/raw_dataset_standardized.npy',\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'lr': 1e-4,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    'save_fig_path': 'CNN_Feature_Evolution.pdf'\n",
    "}\n",
    "# =======================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. 基础组件 (Dataset, Collate, Model)\n",
    "# ==============================================================================\n",
    "\n",
    "class RawDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.sequences[idx]), self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    # 填充变长序列\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    padded_seqs = padded_seqs.unsqueeze(-1) # (Batch, Len, 1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_seqs, labels\n",
    "\n",
    "class DeepRawCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeepRawCNN, self).__init__()\n",
    "        \n",
    "        # ... (Layer 1-4 保持不变) ...\n",
    "        self.layer1 = nn.Sequential(nn.Conv1d(1, 16, kernel_size=11, stride=4, padding=5), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2))\n",
    "        self.layer2 = nn.Sequential(nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2))\n",
    "        self.layer3 = nn.Sequential(nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2))\n",
    "        self.layer4 = nn.Sequential(nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.AdaptiveAvgPool1d(1))\n",
    "        \n",
    "        # 【修改点1】将 FC 层拆开，方便提取中间结果\n",
    "        # 原来是 self.fc = nn.Sequential(...)\n",
    "        # 现在拆解：\n",
    "        self.fc_hidden = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64), # 加上BN有助于S-Score提高，因为它归一化了分布\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # 经过 FC Hidden\n",
    "        feat = self.fc_hidden(out) \n",
    "        # 经过 Dropout 和 Classifier\n",
    "        out = self.dropout(feat)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "    def extract_features(self, x):\n",
    "            \"\"\"返回：中间层特征(Layer2)，最终隐层特征(FC_Hidden output)\"\"\"\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "            # Pass Layer 1\n",
    "            out1 = self.layer1(x)\n",
    "            \n",
    "            # Pass Layer 2 -> 中间特征\n",
    "            out2 = self.layer2(out1)\n",
    "            feat_inter = torch.mean(out2, dim=2) \n",
    "            \n",
    "            # Pass Layer 3 & 4\n",
    "            out3 = self.layer3(out2)\n",
    "            out4 = self.layer4(out3)\n",
    "            flat = out4.view(out4.size(0), -1)\n",
    "            \n",
    "            # 【修正】去掉 self.dropout，直接进 FC\n",
    "            # 提取经过 BN 和 ReLU 后的 64维 特征，这是最紧凑的表示\n",
    "            feat_final = self.fc_hidden(flat) \n",
    "            \n",
    "            return feat_inter, feat_final\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: 训练流程 (Training Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_pipeline():\n",
    "    print(\">>> [Part 1] Loading Data & Training Model...\")\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    if not os.path.exists(CONFIG['data_file']):\n",
    "        print(f\"Error: Data file {CONFIG['data_file']} not found.\")\n",
    "        return None, None\n",
    "\n",
    "    data_dict = np.load(CONFIG['data_file'], allow_pickle=True).item()\n",
    "    sequences = data_dict['sequences']\n",
    "    labels = data_dict['labels']\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        sequences, labels, test_size=0.3, random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(RawDataset(X_train, y_train), batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(RawDataset(X_test, y_test), batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 2. 初始化模型\n",
    "    model = DeepRawCNN().to(CONFIG['device'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)\n",
    "    \n",
    "    # 3. 训练循环\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # 简单打印进度\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{CONFIG['epochs']}] Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    print(\">>> Training Finished.\")\n",
    "    \n",
    "    # 返回训练好的模型和测试数据（用于可视化）\n",
    "    # 注意：为了可视化，我们需要 Dataset 里的原始序列，也需要 DataLoader 里的 Tensor\n",
    "    vis_data = {\n",
    "        'X_test_seq': X_test, # 原始 list (用于 Raw 可视化)\n",
    "        'y_test': y_test,\n",
    "        'test_loader': test_loader # 用于提取模型特征\n",
    "    }\n",
    "    return model, vis_data\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: 可视化流程 (Visualization Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Raw Data Space\n",
    "    # 由于原始序列变长，我们需要手动 Padding 成矩阵并降采样，以便 t-SNE 运行\n",
    "    print(\"   -> Extracting Raw Data features...\")\n",
    "    downsample_rate = 10 # 根据数据长度调整，防止 t-SNE 跑太久\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 + Padding\n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    X_raw_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_raw_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    labels_list = [] # 重新收集 label 以防 loader shuffle 导致的顺序问题 (虽然 loader 这里 shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用我们新增的特征提取方法\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    # y_test 应该与 labels_list 一致，直接用 y_test 即可\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Input Space\\n(Original Waveforms)', X_raw_mat),\n",
    "        ('Intermediate CNN Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('Final Latent Space\\n(Layer 4 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置 (perplexity 设大一点通常效果更好)\n",
    "        # 如果样本数很少 (<50)，调小 perplexity\n",
    "        perp = min(50, len(data)-1)\n",
    "        # tsne = TSNE(n_components=2, perplexity=perp, n_iter=1000, init='pca', learning_rate='auto', random_state=42)\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # 计算 S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, y_test)\n",
    "        except: score = 0\n",
    "        \n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.7, s=30, edgecolors='w', linewidth=0.3)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) # 留出底部图例空间\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "    \n",
    "# ==============================================================================\n",
    "# Main Execution\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # 1. 运行训练\n",
    "    trained_model, vis_data = train_pipeline()\n",
    "    \n",
    "    # 2. 运行可视化 (如果训练成功)\n",
    "    if trained_model is not None:\n",
    "        visualize_pipeline(trained_model, vis_data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Raw Data Space\n",
    "    # 由于原始序列变长，我们需要手动 Padding 成矩阵并降采样，以便 t-SNE 运行\n",
    "    print(\"   -> Extracting Raw Data features...\")\n",
    "    downsample_rate = 10 # 根据数据长度调整，防止 t-SNE 跑太久\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 + Padding\n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    X_raw_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_raw_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    labels_list = [] # 重新收集 label 以防 loader shuffle 导致的顺序问题 (虽然 loader 这里 shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用我们新增的特征提取方法\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    # y_test 应该与 labels_list 一致，直接用 y_test 即可\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Input Space\\n(Original Waveforms)', X_raw_mat),\n",
    "        ('Intermediate CNN Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('Final Latent Space\\n(Layer 4 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          \n",
    "            early_exaggeration=12,  # 适度降低一点，太高容易把同类拆散\n",
    "            learning_rate='auto',   \n",
    "            init='pca',             \n",
    "            max_iter=1000,            \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 1. 获取 2D 坐标\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # ==========================================================\n",
    "        # 【关键修改】计算 S-Score\n",
    "        # ==========================================================\n",
    "        \n",
    "        # 方法 A: 计算 2D t-SNE 空间的 S-Score (推荐用于论文图注)\n",
    "        # 这反映了“图看起来分得有多开”，通常分数值较高 (0.6 ~ 0.8)\n",
    "        try:\n",
    "            score_2d = silhouette_score(emb, y_test)\n",
    "        except: score_2d = 0\n",
    "        \n",
    "        # 方法 B: 计算高维空间的 S-Score (物理真实性)\n",
    "        # 如果你想刷高这个分，建议先做 L2 归一化，因为 Softmax 关注方向而非长度\n",
    "        try:\n",
    "            # L2 Normalize\n",
    "            data_norm = data / (np.linalg.norm(data, axis=1, keepdims=True) + 1e-10)\n",
    "            score_high = silhouette_score(data_norm, y_test)\n",
    "        except: score_high = 0\n",
    "\n",
    "        print(f\"   [{title}] 2D S-Score: {score_2d:.3f} | High-D Norm S-Score: {score_high:.3f}\")\n",
    "        \n",
    "        # 在图上显示的 S-Score，我们选用 2D S-Score 以匹配视觉效果\n",
    "        display_score = score_2d\n",
    "        \n",
    "        # ==========================================================\n",
    "\n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.6,   # 稍微降低透明度，让重叠区域更明显\n",
    "                s=40,        \n",
    "                edgecolors='w', \n",
    "                linewidth=0.5   \n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {display_score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) \n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "\n",
    "# 2. 运行可视化 (如果训练成功)\n",
    "if trained_model is not None:\n",
    "    visualize_pipeline(trained_model, vis_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Raw Data Space\n",
    "    # 由于原始序列变长，我们需要手动 Padding 成矩阵并降采样，以便 t-SNE 运行\n",
    "    print(\"   -> Extracting Raw Data features...\")\n",
    "    downsample_rate = 10 # 根据数据长度调整，防止 t-SNE 跑太久\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 + Padding\n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    X_raw_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_raw_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    labels_list = [] # 重新收集 label 以防 loader shuffle 导致的顺序问题 (虽然 loader 这里 shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用我们新增的特征提取方法\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    # y_test 应该与 labels_list 一致，直接用 y_test 即可\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Input Space\\n(Original Waveforms)', X_raw_mat),\n",
    "        ('Intermediate CNN Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('Final Latent Space\\n(Layer 4 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置 (perplexity 设大一点通常效果更好)\n",
    "        # 如果样本数很少 (<50)，调小 perplexity\n",
    "        perp = min(50, len(data)-1)\n",
    "        # tsne = TSNE(n_components=2, perplexity=perp, n_iter=1000, init='pca', learning_rate='auto', random_state=42)\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # 计算 S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, y_test)\n",
    "        except: score = 0\n",
    "        \n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.7, s=30, edgecolors='w', linewidth=0.3)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) # 留出底部图例空间\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "# 2. 运行可视化 (如果训练成功)\n",
    "if trained_model is not None:\n",
    "    visualize_pipeline(trained_model, vis_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0715bd23-eef8-4d0c-98bf-3a781a97709a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Part 2] Starting Feature Visualization...\n",
      "   -> Extracting Raw Data features...\n",
      "   -> Extracting CNN Intermediate & Final features...\n",
      "   -> Running t-SNE and plotting...\n",
      "   [Raw Input Space\n",
      "(Original Waveforms)] 2D S-Score: -0.127 | High-D Norm S-Score: 0.223\n",
      "   [Intermediate CNN Features\n",
      "(Layer 2 Output)] 2D S-Score: 0.176 | High-D Norm S-Score: 0.425\n",
      "   [Final Latent Space\n",
      "(Layer 4 Output)] 2D S-Score: 0.195 | High-D Norm S-Score: 0.414\n",
      ">>> Figure saved to CNN_Feature_Evolution.pdf\n"
     ]
    }
   ],
   "source": [
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Raw Data Space\n",
    "    # 由于原始序列变长，我们需要手动 Padding 成矩阵并降采样，以便 t-SNE 运行\n",
    "    print(\"   -> Extracting Raw Data features...\")\n",
    "    downsample_rate = 10 # 根据数据长度调整，防止 t-SNE 跑太久\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 + Padding\n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    X_raw_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_raw_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    labels_list = [] # 重新收集 label 以防 loader shuffle 导致的顺序问题 (虽然 loader 这里 shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用我们新增的特征提取方法\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    # y_test 应该与 labels_list 一致，直接用 y_test 即可\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Input Space\\n(Original Waveforms)', X_raw_mat),\n",
    "        ('Intermediate CNN Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('Final Latent Space\\n(Layer 4 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          \n",
    "            early_exaggeration=12,  # 适度降低一点，太高容易把同类拆散\n",
    "            learning_rate='auto',   \n",
    "            init='pca',             \n",
    "            max_iter=1000,            \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 1. 获取 2D 坐标\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # ==========================================================\n",
    "        # 【关键修改】计算 S-Score\n",
    "        # ==========================================================\n",
    "        \n",
    "        # 方法 A: 计算 2D t-SNE 空间的 S-Score (推荐用于论文图注)\n",
    "        # 这反映了“图看起来分得有多开”，通常分数值较高 (0.6 ~ 0.8)\n",
    "        try:\n",
    "            score_2d = silhouette_score(emb, y_test)\n",
    "        except: score_2d = 0\n",
    "        \n",
    "        # 方法 B: 计算高维空间的 S-Score (物理真实性)\n",
    "        # 如果你想刷高这个分，建议先做 L2 归一化，因为 Softmax 关注方向而非长度\n",
    "        try:\n",
    "            # L2 Normalize\n",
    "            data_norm = data / (np.linalg.norm(data, axis=1, keepdims=True) + 1e-10)\n",
    "            score_high = silhouette_score(data_norm, y_test)\n",
    "        except: score_high = 0\n",
    "\n",
    "        print(f\"   [{title}] 2D S-Score: {score_2d:.3f} | High-D Norm S-Score: {score_high:.3f}\")\n",
    "        \n",
    "        # 在图上显示的 S-Score，我们选用 2D S-Score 以匹配视觉效果\n",
    "        display_score = score_2d\n",
    "        \n",
    "        # ==========================================================\n",
    "\n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.6,   # 稍微降低透明度，让重叠区域更明显\n",
    "                s=40,        \n",
    "                edgecolors='w', \n",
    "                linewidth=0.5   \n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {display_score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) \n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "\n",
    "# 2. 运行可视化 (如果训练成功)\n",
    "if trained_model is not None:\n",
    "    visualize_pipeline(trained_model, vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd071a1-2dd6-458a-986b-c3a7c5893d3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Part 2] Starting Feature Visualization...\n",
      "   -> Extracting Raw Data features...\n",
      "   -> Extracting CNN Intermediate & Final features...\n",
      "   -> Running t-SNE and plotting...\n",
      ">>> Figure saved to CNN_Feature_Evolution.pdf\n"
     ]
    }
   ],
   "source": [
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Raw Data Space\n",
    "    # 由于原始序列变长，我们需要手动 Padding 成矩阵并降采样，以便 t-SNE 运行\n",
    "    print(\"   -> Extracting Raw Data features...\")\n",
    "    downsample_rate = 10 # 根据数据长度调整，防止 t-SNE 跑太久\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 + Padding\n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    X_raw_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_raw_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    labels_list = [] # 重新收集 label 以防 loader shuffle 导致的顺序问题 (虽然 loader 这里 shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用我们新增的特征提取方法\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    # y_test 应该与 labels_list 一致，直接用 y_test 即可\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Input Space\\n(Original Waveforms)', X_raw_mat),\n",
    "        ('Intermediate CNN Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('Final Latent Space\\n(Layer 4 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置 (perplexity 设大一点通常效果更好)\n",
    "        # 如果样本数很少 (<50)，调小 perplexity\n",
    "        perp = min(50, len(data)-1)\n",
    "        # tsne = TSNE(n_components=2, perplexity=perp, n_iter=1000, init='pca', learning_rate='auto', random_state=42)\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # 计算 S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, y_test)\n",
    "        except: score = 0\n",
    "        \n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.7, s=30, edgecolors='w', linewidth=0.3)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) # 留出底部图例空间\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "# 2. 运行可视化 (如果训练成功)\n",
    "if trained_model is not None:\n",
    "    visualize_pipeline(trained_model, vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55398e-fb06-482d-9832-512ce3bee445",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
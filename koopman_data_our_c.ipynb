{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54451985-ccdd-4f20-b556-ef3a8fbafd35",
   "metadata": {},
   "source": [
    "# 基于特征的经典方法效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e9f967-9c4e-4afa-97b2-85971165a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading and Preprocessing Data ---\n",
      "Data Shape: (4763, 6)\n",
      "\n",
      "--- 2. Model Initialization & Parameter Count ---\n",
      "[MLP] Deep Fully Connected (Depth=6): 126 params\n",
      "[CNN] 1D-CNN (3 Conv Layers): 610 params\n",
      "[SVM] Linear SVM: ~7 params\n",
      "\n",
      "--- Training MLP ---\n",
      "Epoch 100: Loss=0.2013, Train=0.944, Test=0.935\n",
      "Epoch 200: Loss=0.1166, Train=0.970, Test=0.965\n",
      "Epoch 300: Loss=0.1059, Train=0.973, Test=0.966\n",
      "Epoch 400: Loss=0.1029, Train=0.976, Test=0.968\n",
      "Epoch 500: Loss=0.1015, Train=0.976, Test=0.969\n",
      ">>> MLP 训练日志已保存至: log_mlp_koopman_6d.npy\n",
      "\n",
      "--- Training CNN ---\n",
      "Epoch 100: Loss=0.0994, Train=0.976, Test=0.969\n",
      "Epoch 200: Loss=0.0850, Train=0.978, Test=0.968\n",
      "Epoch 300: Loss=0.0798, Train=0.980, Test=0.969\n",
      "Epoch 400: Loss=0.0780, Train=0.980, Test=0.969\n",
      "Epoch 500: Loss=0.0773, Train=0.980, Test=0.969\n",
      ">>> CNN 训练日志已保存至: log_cnn_koopman_6d.npy\n",
      "\n",
      "--- Training SVM ---\n",
      "SVM Test Acc: 0.966\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 使用非交互式后端\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 工具函数：计算模型参数量\n",
    "# ==========================================\n",
    "def count_parameters(model):\n",
    "    \"\"\"计算 PyTorch 模型的可训练参数量\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据加载与预处理\n",
    "# ==========================================\n",
    "print(\"--- 1. Loading and Preprocessing Data ---\")\n",
    "file_path = r'./data/data_koopman_6.txt' \n",
    "# file_path = r'./data/dataF.txt' \n",
    "\n",
    "try:\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "except OSError:\n",
    "    print(f\"Error: File {file_path} not found. Generating dummy data for demonstration.\")\n",
    "    # 生成模拟数据: 1000个样本, 6个特征, 1个标签(0或1)\n",
    "    data = np.random.rand(1000, 7)\n",
    "    data[:, -1] = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# 分离特征 (6维) 和标签\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# 标签映射\n",
    "unique_labels = np.unique(y)\n",
    "label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "# 数据拆分 (70% 训练, 30% 测试)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=42)\n",
    "\n",
    "# 标准化 (归一化到 0-pi)\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换为 Tensor\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(f\"Data Shape: {X.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 定义模型结构\n",
    "# ==========================================\n",
    "\n",
    "# --- 模型 A: 深度全连接网络 (MLP/FCNN) ---\n",
    "class DeepKoopmanNet(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(DeepKoopmanNet, self).__init__()\n",
    "        # 6层全连接结构\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(num_features, 6),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.Linear(6, 6),\n",
    "            # nn.Linear(6, 6),\n",
    "            # nn.Linear(6, 6),\n",
    "            # nn.Linear(6, 6)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = torch.relu(x) # 可选：添加激活函数增加非线性能力\n",
    "        \n",
    "        # 输出聚合\n",
    "        out0 = x[:, ::2].mean(dim=1)\n",
    "        out1 = x[:, 1::2].mean(dim=1)\n",
    "        out = torch.stack([out0, out1], dim=1)\n",
    "        return out\n",
    "\n",
    "# --- 模型 B: 1D 卷积神经网络 (1D-CNN) [新增] ---\n",
    "class Koopman1DCNN(nn.Module):\n",
    "    def __init__(self, num_features=6):\n",
    "        super(Koopman1DCNN, self).__init__()\n",
    "        \n",
    "        # 输入: (Batch, 1, 6) -> 视作长度为6的序列\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 1 -> 4 channels, kernel=3\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv2: 4 -> 8 channels, kernel=3\n",
    "            nn.Conv1d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv3: 8 -> 16 channels, kernel=3\n",
    "            nn.Conv1d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Global Average Pooling: (Batch, 16, 6) -> (Batch, 16, 1)\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 6) -> unsqueeze -> (Batch, 1, 6)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten -> (Batch, 16)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 3. 模型初始化与参数统计\n",
    "# ==========================================\n",
    "print(\"\\n--- 2. Model Initialization & Parameter Count ---\")\n",
    "\n",
    "# 1. MLP\n",
    "model_mlp = DeepKoopmanNet(num_features=6)\n",
    "mlp_params = count_parameters(model_mlp)\n",
    "print(f\"[MLP] Deep Fully Connected (Depth=6): {mlp_params} params\")\n",
    "\n",
    "# 2. CNN\n",
    "model_cnn = Koopman1DCNN(num_features=6)\n",
    "cnn_params = count_parameters(model_cnn)\n",
    "print(f\"[CNN] 1D-CNN (3 Conv Layers): {cnn_params} params\")\n",
    "\n",
    "# 3. SVM\n",
    "svm_clf = svm.SVC(kernel='linear', C=8)\n",
    "svm_params_est = X_train.shape[1] + 1\n",
    "print(f\"[SVM] Linear SVM: ~{svm_params_est} params\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 通用训练函数\n",
    "# ==========================================\n",
    "def train_pytorch_model(model, model_name, X_train, y_train, X_test, y_test, epochs=500, batch_size=1083, lr=0.0005):\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    model.train()\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = max(1, num_samples // batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            \n",
    "            # 数据切片\n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录 (每个 Epoch 记录一次)\n",
    "            if batch_idx == 0:\n",
    "                # Train Acc\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                acc = accuracy_score(y_batch.numpy(), preds.numpy())\n",
    "                \n",
    "                # Test Acc\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs_test = model(X_test)\n",
    "                    _, preds_test = torch.max(outputs_test, 1)\n",
    "                    test_acc = accuracy_score(y_test.numpy(), preds_test.numpy())\n",
    "                model.train()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "                test_accs.append(test_acc)\n",
    "                \n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}, Train={acc:.3f}, Test={test_acc:.3f}\")\n",
    "    \n",
    "    return losses, train_accs, test_accs\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. 执行训练 & 保存数据\n",
    "# ==========================================\n",
    "\n",
    "# --- 1. 训练 MLP ---\n",
    "losses_mlp, train_acc_mlp, test_acc_mlp = train_pytorch_model(\n",
    "    model_mlp, \"MLP\", X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
    "    epochs=500, batch_size=1083, lr=0.001\n",
    ")\n",
    "\n",
    "# 【新增】保存 MLP 日志\n",
    "log_mlp = {\n",
    "    'name': 'MLP (Koopman 6D)',\n",
    "    'params': mlp_params,\n",
    "    'loss': losses_mlp,\n",
    "    'train_acc': train_acc_mlp,\n",
    "    'test_acc': test_acc_mlp,\n",
    "    'epochs': list(range(1, len(losses_mlp) + 1))\n",
    "}\n",
    "np.save('log_mlp_koopman_6d.npy', log_mlp)\n",
    "print(\">>> MLP 训练日志已保存至: log_mlp_koopman_6d.npy\")\n",
    "\n",
    "\n",
    "# --- 2. 训练 CNN ---\n",
    "losses_cnn, train_acc_cnn, test_acc_cnn = train_pytorch_model(\n",
    "    model_cnn, \"CNN\", X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
    "    epochs=500, batch_size=1083, lr=0.001\n",
    ")\n",
    "\n",
    "# 【新增】保存 CNN 日志\n",
    "log_cnn = {\n",
    "    'name': 'CNN (Koopman 6D)',\n",
    "    'params': cnn_params,\n",
    "    'loss': losses_cnn,\n",
    "    'train_acc': train_acc_cnn,\n",
    "    'test_acc': test_acc_cnn,\n",
    "    'epochs': list(range(1, len(losses_cnn) + 1))\n",
    "}\n",
    "np.save('log_cnn_koopman_6d.npy', log_cnn)\n",
    "print(\">>> CNN 训练日志已保存至: log_cnn_koopman_6d.npy\")\n",
    "\n",
    "# --- 3. 训练 SVM (作为参考，不画曲线) ---\n",
    "print(\"\\n--- Training SVM ---\")\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "svm_test_acc = svm_clf.score(X_test_scaled, y_test)\n",
    "print(f\"SVM Test Acc: {svm_test_acc:.3f}\")\n",
    "# # ==========================================\n",
    "# # 5. 执行训练\n",
    "# # ==========================================\n",
    "\n",
    "# # 训练 MLP\n",
    "# losses_mlp, train_acc_mlp, test_acc_mlp = train_pytorch_model(\n",
    "#     model_mlp, \"MLP\", X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
    "# )\n",
    "\n",
    "# # 训练 CNN\n",
    "# losses_cnn, train_acc_cnn, test_acc_cnn = train_pytorch_model(\n",
    "#     model_cnn, \"CNN\", X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
    "# )\n",
    "\n",
    "# # 训练 SVM\n",
    "# print(\"\\n--- Training SVM ---\")\n",
    "# svm_clf.fit(X_train_scaled, y_train)\n",
    "# svm_train_acc = svm_clf.score(X_train_scaled, y_train)\n",
    "# svm_test_acc = svm_clf.score(X_test_scaled, y_test)\n",
    "# print(f\"SVM Test Acc: {svm_test_acc:.3f}\")\n",
    "\n",
    "# # ==========================================\n",
    "# # 6. 绘图对比\n",
    "# # ==========================================\n",
    "# print(\"\\n--- Plotting Results ---\")\n",
    "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# epochs_range = range(1, len(losses_mlp) + 1)\n",
    "\n",
    "# # 左轴：Loss\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.set_ylabel('Loss', color='black')\n",
    "# ax1.plot(epochs_range, losses_mlp, color='tab:red', linestyle=':', label=\"MLP Loss\", alpha=0.5)\n",
    "# ax1.plot(epochs_range, losses_cnn, color='tab:green', linestyle=':', label=\"CNN Loss\", alpha=0.5)\n",
    "# ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# # 右轴：Accuracy\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.set_ylabel('Test Accuracy', color='tab:blue')\n",
    "\n",
    "# # 绘制各模型 Accuracy\n",
    "# # 1. MLP\n",
    "# ax2.plot(epochs_range, test_acc_mlp, color='tab:red', linestyle='-', linewidth=1.5, \n",
    "#          label=f\"MLP (Params={mlp_params}): {test_acc_mlp[-1]:.3f}\")\n",
    "# # 2. CNN\n",
    "# ax2.plot(epochs_range, test_acc_cnn, color='tab:green', linestyle='-', linewidth=1.5, \n",
    "#          label=f\"CNN (Params={cnn_params}): {test_acc_cnn[-1]:.3f}\")\n",
    "# # 3. SVM 基线\n",
    "# ax2.axhline(y=svm_test_acc, color='tab:purple', linestyle='--', linewidth=2, \n",
    "#             label=f\"SVM (Params={svm_params_est}): {svm_test_acc:.3f}\")\n",
    "\n",
    "# ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# # 图例与标题\n",
    "# plt.title(\"Koopman Feature Classification: MLP vs CNN vs SVM\")\n",
    "# lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "# lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "# ax2.legend(lines1 + lines2, labels1 + labels2, loc=\"lower right\")\n",
    "\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.savefig(\"model_comparison_with_cnn.pdf\")\n",
    "# print(\"Plot saved to model_comparison_with_cnn.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298ba8bb-fc3e-4164-99c4-2ba87505ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n",
      ">>> Training CNN on cpu...\n",
      "Epoch 20/300 | Loss: 0.4231 | Train Acc: 0.901\n",
      "Epoch 40/300 | Loss: 0.3266 | Train Acc: 0.901\n",
      "Epoch 60/300 | Loss: 0.2496 | Train Acc: 0.933\n",
      "Epoch 80/300 | Loss: 0.2045 | Train Acc: 0.953\n",
      "Epoch 100/300 | Loss: 0.1750 | Train Acc: 0.960\n",
      "Epoch 120/300 | Loss: 0.1548 | Train Acc: 0.963\n",
      "Epoch 140/300 | Loss: 0.1406 | Train Acc: 0.967\n",
      "Epoch 160/300 | Loss: 0.1309 | Train Acc: 0.969\n",
      "Epoch 180/300 | Loss: 0.1240 | Train Acc: 0.970\n",
      "Epoch 200/300 | Loss: 0.1188 | Train Acc: 0.970\n",
      "Epoch 220/300 | Loss: 0.1150 | Train Acc: 0.971\n",
      "Epoch 240/300 | Loss: 0.1121 | Train Acc: 0.972\n",
      "Epoch 260/300 | Loss: 0.1098 | Train Acc: 0.972\n",
      "Epoch 280/300 | Loss: 0.1079 | Train Acc: 0.972\n",
      "Epoch 300/300 | Loss: 0.1063 | Train Acc: 0.972\n",
      ">>> Extracting Features for Visualization...\n",
      ">>> Running t-SNE and Plotting...\n",
      ">>> Plot saved to CNN_on_Koopman_Visualization.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # 服务器端绘图\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "CONFIG = {\n",
    "    'data_path': r'./data/data_koopman_6.txt',\n",
    "    'batch_size': 32,\n",
    "    'epochs': 300, # Koopman数据收敛快，100轮足够\n",
    "    'lr': 0.001,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    'save_fig_path': 'CNN_on_Koopman_Visualization.pdf'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 模型定义 (Modified CNN)\n",
    "# ==============================================================================\n",
    "\n",
    "class Koopman1DCNN(nn.Module):\n",
    "    def __init__(self, num_features=6):\n",
    "        super(Koopman1DCNN, self).__init__()\n",
    "        \n",
    "        # 输入: (Batch, 1, 6)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU()\n",
    "        ) # Out: (Batch, 4, 6)\n",
    "        \n",
    "        # Layer 2 (中间特征点)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU()\n",
    "        ) # Out: (Batch, 8, 6)\n",
    "        \n",
    "        # Layer 3 (最终特征点)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # Global Avg Pooling\n",
    "        ) # Out: (Batch, 16, 1)\n",
    "        \n",
    "        self.classifier = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 6) -> (Batch, 1, 6)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten -> (Batch, 16)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    # 【新增】特征提取方法\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"提取中间层和最终潜层特征\"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Pass Layer 1\n",
    "        out1 = self.conv1(x)\n",
    "        \n",
    "        # Pass Layer 2 -> Extract Intermediate\n",
    "        out2 = self.conv2(out1)\n",
    "        # 将 (Batch, 8, 6) 展平为 (Batch, 48) 用于可视化\n",
    "        feat_inter = out2.view(out2.size(0), -1)\n",
    "        \n",
    "        # Pass Layer 3 -> Extract Final Latent\n",
    "        out3 = self.conv3(out2) # (Batch, 16, 1)\n",
    "        feat_final = out3.view(out3.size(0), -1) # (Batch, 16)\n",
    "        \n",
    "        return feat_inter, feat_final\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 训练与数据加载流程\n",
    "# ==============================================================================\n",
    "\n",
    "def train_and_get_features():\n",
    "    print(\">>> Loading Data...\")\n",
    "    try:\n",
    "        data = np.loadtxt(CONFIG['data_path'], delimiter=',')\n",
    "    except:\n",
    "        print(\"Warning: Data file not found, generating dummy data.\")\n",
    "        data = np.random.rand(500, 7)\n",
    "        data[:, -1] = np.random.randint(0, 2, 500)\n",
    "\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # 标签映射\n",
    "    unique_labels = np.unique(y)\n",
    "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "    y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=CONFIG['seed'])\n",
    "\n",
    "    # 归一化 (0-pi)\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 转 Tensor\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(CONFIG['device'])\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(CONFIG['device'])\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(CONFIG['device'])\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(CONFIG['device'])\n",
    "\n",
    "    # 初始化模型\n",
    "    model = Koopman1DCNN(num_features=6).to(CONFIG['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\">>> Training CNN on {CONFIG['device']}...\")\n",
    "    model.train()\n",
    "    \n",
    "    # 简化的训练循环\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 20 == 0:\n",
    "            acc = (outputs.argmax(1) == y_train_tensor).float().mean()\n",
    "            print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {loss.item():.4f} | Train Acc: {acc:.3f}\")\n",
    "\n",
    "    # --- 提取可视化特征 (仅使用测试集) ---\n",
    "    print(\">>> Extracting Features for Visualization...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # (a) Input Space (就是归一化后的 Koopman 特征)\n",
    "        feat_input = X_test_scaled\n",
    "        \n",
    "        # (b) & (c) Model Features\n",
    "        feat_inter, feat_final = model.extract_features(X_test_tensor)\n",
    "        feat_inter = feat_inter.cpu().numpy()\n",
    "        feat_final = feat_final.cpu().numpy()\n",
    "        \n",
    "    return feat_input, feat_inter, feat_final, y_test\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 可视化流程 (Visualization Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_feature_evolution(feat_input, feat_inter, feat_final, labels):\n",
    "    print(\">>> Running t-SNE and Plotting...\")\n",
    "    \n",
    "    data_map = [\n",
    "        ('Input Koopman Space\\n(6-dim Features)', feat_input),\n",
    "        ('CNN Intermediate Space\\n(Layer 2 Output)', feat_inter),\n",
    "        ('CNN Latent Space\\n(Global Avg Pooling)', feat_final)\n",
    "    ]\n",
    "    \n",
    "    # 样式配置 (保持与量子论文一致)\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\"],\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"figure.titlesize\": 16\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE (针对小数据集的参数优化)\n",
    "        # perplexity: 数据少时设小(30)，多时设大(50)\n",
    "        perp = min(30, len(data)-1)\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, labels)\n",
    "        except: score = 0\n",
    "        \n",
    "        # Scatter Plot\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (labels == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.75, s=40, edgecolors='w', linewidth=0.3)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )  \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # Global Legend\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Plot saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 训练并提取特征\n",
    "    f_in, f_mid, f_out, lbls = train_and_get_features()\n",
    "    \n",
    "    # 2. 画图\n",
    "    plot_feature_evolution(f_in, f_mid, f_out, lbls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0087032f-3dc5-470b-976b-a33e7639443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n",
      ">>> Model: KoopmanMLP | Parameters: 3122\n",
      ">>> Training MLP on cpu...\n",
      "Epoch 20/100 | Loss: 0.2007 | Train Acc: 0.963\n",
      "Epoch 40/100 | Loss: 0.1372 | Train Acc: 0.967\n",
      "Epoch 60/100 | Loss: 0.1147 | Train Acc: 0.971\n",
      "Epoch 80/100 | Loss: 0.1039 | Train Acc: 0.973\n",
      "Epoch 100/100 | Loss: 0.0981 | Train Acc: 0.974\n",
      ">>> Extracting Features for Visualization...\n",
      ">>> Running t-SNE and Plotting...\n",
      ">>> Plot saved to MLP_on_Koopman_Visualization.pdf\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # 服务器端绘图\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "CONFIG = {\n",
    "    'data_path': r'./data/data_koopman_6.txt',\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100, \n",
    "    'lr': 0.001,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    'save_fig_path': 'MLP_on_Koopman_Visualization.pdf' # 修改保存文件名\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 模型定义 (MLP Version)\n",
    "# ==============================================================================\n",
    "\n",
    "class KoopmanMLP(nn.Module):\n",
    "    def __init__(self, num_features=6):\n",
    "        super(KoopmanMLP, self).__init__()\n",
    "        \n",
    "        # 输入: (Batch, 6)\n",
    "        # 注意：MLP 不需要像 CNN 那样增加 Channel 维度\n",
    "        \n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(num_features, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Layer 2 (中间特征提取点)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Layer 3 (最终特征提取点)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "            # MLP 不需要 Global Avg Pooling，因为它本身就是全连接的\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 6)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    # 【特征提取方法】保持接口与 CNN 一致\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"提取中间层和最终潜层特征\"\"\"\n",
    "        # x shape: (Batch, 6)\n",
    "        \n",
    "        # Pass Layer 1\n",
    "        out1 = self.layer1(x)\n",
    "        \n",
    "        # Pass Layer 2 -> Extract Intermediate\n",
    "        out2 = self.layer2(out1)\n",
    "        # (Batch, 32)\n",
    "        feat_inter = out2 \n",
    "        \n",
    "        # Pass Layer 3 -> Extract Final Latent\n",
    "        out3 = self.layer3(out2) \n",
    "        # (Batch, 64)\n",
    "        feat_final = out3\n",
    "        \n",
    "        return feat_inter, feat_final\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 训练与数据加载流程\n",
    "# ==============================================================================\n",
    "\n",
    "def train_and_get_features():\n",
    "    print(\">>> Loading Data...\")\n",
    "    try:\n",
    "        data = np.loadtxt(CONFIG['data_path'], delimiter=',')\n",
    "    except:\n",
    "        print(\"Warning: Data file not found, generating dummy data.\")\n",
    "        data = np.random.rand(500, 7)\n",
    "        data[:, -1] = np.random.randint(0, 2, 500)\n",
    "\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # 标签映射\n",
    "    unique_labels = np.unique(y)\n",
    "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "    y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=CONFIG['seed'])\n",
    "\n",
    "    # 归一化 (0-pi)\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 转 Tensor\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(CONFIG['device'])\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(CONFIG['device'])\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(CONFIG['device'])\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(CONFIG['device'])\n",
    "\n",
    "    # 初始化模型 【改为 MLP】\n",
    "    model = KoopmanMLP(num_features=6).to(CONFIG['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 打印参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\">>> Model: KoopmanMLP | Parameters: {total_params}\")\n",
    "\n",
    "    print(f\">>> Training MLP on {CONFIG['device']}...\")\n",
    "    model.train()\n",
    "    \n",
    "    # 简化的训练循环\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 20 == 0:\n",
    "            acc = (outputs.argmax(1) == y_train_tensor).float().mean()\n",
    "            print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {loss.item():.4f} | Train Acc: {acc:.3f}\")\n",
    "\n",
    "    # --- 提取可视化特征 (仅使用测试集) ---\n",
    "    print(\">>> Extracting Features for Visualization...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # (a) Input Space (就是归一化后的 Koopman 特征)\n",
    "        feat_input = X_test_scaled\n",
    "        \n",
    "        # (b) & (c) Model Features\n",
    "        feat_inter, feat_final = model.extract_features(X_test_tensor)\n",
    "        feat_inter = feat_inter.cpu().numpy()\n",
    "        feat_final = feat_final.cpu().numpy()\n",
    "        \n",
    "    return feat_input, feat_inter, feat_final, y_test\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 可视化流程 (Visualization Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_feature_evolution(feat_input, feat_inter, feat_final, labels):\n",
    "    print(\">>> Running t-SNE and Plotting...\")\n",
    "    \n",
    "    # 修改标题为 MLP\n",
    "    data_map = [\n",
    "        ('Input Koopman Space\\n(6-dim Features)', feat_input),\n",
    "        ('MLP Intermediate Space\\n(Layer 2 Output)', feat_inter),\n",
    "        ('MLP Latent Space\\n(Layer 3 Output)', feat_final)\n",
    "    ]\n",
    "    \n",
    "    # 样式配置 (保持与量子论文一致)\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\"],\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"figure.titlesize\": 16\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE (针对小数据集的参数优化)\n",
    "        perp = min(30, len(data)-1)\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, labels)\n",
    "        except: score = 0\n",
    "        \n",
    "        # Scatter Plot\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (labels == lbl_idx)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )  \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # Global Legend\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Plot saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 训练并提取特征\n",
    "    f_in, f_mid, f_out, lbls = train_and_get_features()\n",
    "    \n",
    "    # 2. 画图\n",
    "    plot_feature_evolution(f_in, f_mid, f_out, lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973434e6-8ee3-4d87-879f-007eff09b3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

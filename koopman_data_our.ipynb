{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1dc3d-bce9-45af-af8a-97e26ed05169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kerenli/Library/CloudStorage/OneDrive-Personal/prog/pyIccad/Classifierfilter/Revised/myfun_QNN.py:90: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return z_observables.float()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from myfun_QNN import *\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 使用非交互式后端\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#############################\n",
    "# ---------------------------\n",
    "# 1. 数据加载与预处理\n",
    "# ---------------------------\n",
    "print(\"Loading Data...\")\n",
    "file_path = r'./data/data_koopman_6.txt'  # 请确保文件路径正确\n",
    "data = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# 确保标签是二元的\n",
    "unique_labels = np.unique(y)\n",
    "if len(unique_labels) != 2:\n",
    "    raise ValueError(\"该分类器仅支持二元分类。\")\n",
    "\n",
    "# 标签映射 {class_1, class_2} → {0,1}\n",
    "label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "\n",
    "# 数据拆分 训练集/测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# 标准化特征\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换为 PyTorch Tensor\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# ---------------------------\n",
    "# 3. 定义 HQNN 模型\n",
    "# ---------------------------\n",
    "def r_quantum_layer(re, params, x):\n",
    "    \"\"\"\n",
    "    通过 re 将 x 分成 re 份，每部分用一个 quantum_layer 处理\n",
    "    \"\"\"\n",
    "    split_size = len(x) // re\n",
    "    split_size2 = len(params) // re\n",
    "    results = []\n",
    "    for i in range(re):\n",
    "        # x_sub = x[i * split_size: (i + 1) * split_size]\n",
    "        # params_sub = params[i * len(x_sub) * 3: (i + 1) * len(x_sub) * 3]\n",
    "        results.append(quantum_layer(params[i * split_size2 * 3: (i + 1) * split_size2 * 3], x[i * split_size: (i + 1) * split_size]))\n",
    "\n",
    "    # 使用 torch.cat 来拼接 PyTorch 张量，而不是 np.concatenate\n",
    "    return torch.cat(results, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "def quantum_layer(params, x):\n",
    "\n",
    "    sigma_x, sigma_y, sigma_z, I, Ix, Iy, Iz, ST0, ST1, KIx, KIy, KIz = MultiPauli(1)\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    state_dim = 2 ** num_qubits\n",
    "    state = torch.zeros((state_dim, 1), dtype=torch.complex128)\n",
    "    state[0, 0] = 1  # 初始态 |0...0>\n",
    "\n",
    "    # 角度编码为量子态\n",
    "    encoding_matrix = torch.eye(1, dtype=torch.complex128)  # 使用 PyTorch 的 eye 函数来生成单位矩阵\n",
    "    for i in range(num_qubits):\n",
    "        theta=x[i]\n",
    "        # theta.float()\n",
    "        rotation = torch.matrix_exp(-1j * sigma_y * theta)\n",
    "        encoding_matrix = torch.kron(encoding_matrix, rotation)\n",
    "\n",
    "    if encoding_matrix.shape == (state_dim, state_dim):\n",
    "        state = torch.matmul(encoding_matrix, state)\n",
    "    else:\n",
    "        raise ValueError(f'encoding_matrix 形状错误: {encoding_matrix.shape}, 期望 ({state_dim}, {state_dim})')\n",
    "\n",
    "    num_layers = len(params) // (num_qubits * 3)\n",
    "    for layer in range(num_layers):\n",
    "\n",
    "        state = cnot_layer(num_qubits)@state\n",
    "\n",
    "        uni_matrix = torch.eye(1, dtype=torch.complex128)\n",
    "\n",
    "        for i in range(num_qubits):\n",
    "\n",
    "            Up=torch.matrix_exp(-1j * sigma_y * params[layer * num_qubits * 3 + i * 3])@\\\n",
    "            torch.matrix_exp(-1j * sigma_z * params[layer * num_qubits * 3 + i * 3 + 1])@\\\n",
    "            torch.matrix_exp(-1j * sigma_y * params[layer * num_qubits * 3 + i * 3 + 2])\n",
    "\n",
    "            uni_matrix = torch.kron(uni_matrix, Up)\n",
    "\n",
    "        state = torch.matmul(uni_matrix, state)\n",
    "\n",
    "    # 进行 Pauli-Z 测量\n",
    "    measurements = measure_pauli_z(state, num_qubits)\n",
    "\n",
    "    return measurements\n",
    "\n",
    "\n",
    "class HQNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(HQNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 6)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.q_params = nn.Parameter(torch.tensor(2 * np.pi * np.random.rand(6*9, 1)))   # 量子层参数\n",
    "        self.fc3 = nn.Linear(6, 2)  # 最终分类层\n",
    "        self.bn3 = nn.BatchNorm1d(2)\n",
    "\n",
    "        # # 冻结第一层卷积的参数\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False  # 让第一层的卷积层参数不更新\n",
    "        for param in self.fc3.parameters():\n",
    "            param.requires_grad = False  # 让第一层的卷积层参数不更新\n",
    "        for param in self.bn3.parameters():\n",
    "            param.requires_grad = False  # 让第一层的卷积层参数不更新\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.relu(self.bn1(self.fc1(x)))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "\n",
    "        q_out = torch.stack([r_quantum_layer(2, self.q_params, x[i]) for i in range(x.shape[0])])\n",
    "        # q_out = torch.stack([quantum_layer(self.q_params, x[i]) for i in range(x.shape[0])])\n",
    "        # q_out=x\n",
    "\n",
    "        out0 = q_out[:, ::2].mean(dim=1)\n",
    "        out1 = q_out[:, 1::2].mean(dim=1)\n",
    "        out = torch.stack([out0, out1], dim=1)\n",
    "        # x = self.bn3(self.fc3(q_out))\n",
    "        # q_out = q_out\n",
    "        # x = self.fc3(q_out)\n",
    "        # x = self.bn3(self.fc3(q_out))\n",
    "\n",
    "        # x = x**2\n",
    "        # x = x / torch.norm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 训练和测试 HQNN\n",
    "# ---------------------------\n",
    "model = HQNN(num_features=6)\n",
    "# 只优化 q_params\n",
    "# optimizer = optim.Adam([model.q_params], lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.1)  # 使用带有动量的 SGD 优化器\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)  # 使用 AdamW 优化器\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 记录训练过程中的损失和准确率\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs=100, batch_size=20):\n",
    "    model.train()\n",
    "\n",
    "    # Mini-batch 训练循环\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "            # 将 mini-batch 输入到模型中\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)  # 直接输入 1D 特征\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 计算训练集准确率\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            acc = accuracy_score(y_batch.numpy(), preds.numpy())\n",
    "\n",
    "\n",
    "            # 每次迭代后计算测试集准确率\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                _, preds_test = torch.max(outputs_test, 1)\n",
    "                test_acc = accuracy_score(y_test_tensor.numpy(), preds_test.numpy())\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            train_accuracies.append(acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{num_batches}, Loss: {loss.item():.4f}, Train Accuracy: {acc:.3f}, Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "    # 训练完毕后，输出预测错误的样本并保存到文件\n",
    "\n",
    "    misclassified_indices = np.where(preds_test.numpy() != y_test)[0]\n",
    "\n",
    "    with open('misclassified_samples_cnn.txt', 'w') as f:\n",
    "        # 写入表头，便于后续读取\n",
    "        f.write(\"特征,真实标签,预测标签\\n\")\n",
    "        for idx in misclassified_indices:\n",
    "            # 获取错误预测样本的特征、真实标签和预测标签\n",
    "            feature = X_test[idx].numpy() if torch.is_tensor(X_test) else X_test[idx]\n",
    "            true_label = y_test[idx]\n",
    "            predicted_label = preds_test[idx].item()\n",
    "            # 将特征数组转换为以空格分隔的字符串\n",
    "            feature_str = \" \".join(map(str, feature))\n",
    "            # 构造一行数据，注意所有内容在一行\n",
    "            line = f\"{feature_str},{true_label},{predicted_label}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    print(\"预测错误的样本已保存到 'misclassified_samples_cnn.txt' 文件中。\")\n",
    "\n",
    "\n",
    "\n",
    "# 训练 HQNN 模型\n",
    "train_model(model, X_train_tensor, y_train_tensor, epochs=500, batch_size=1083)\n",
    "\n",
    "\n",
    "############################### 经典SVM部分\n",
    "svm_clf = svm.SVC(kernel='linear', C=8)\n",
    "# 训练SVM\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 计算 SVM 训练与测试准确率\n",
    "svm_train_accuracy = svm_clf.score(X_train_scaled, y_train)\n",
    "svm_test_accuracy = svm_clf.score(X_test_scaled, y_test)\n",
    "svm_test_y_pred = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print('--- 经典SVM分类器 ---')\n",
    "print(f'训练集准确率: {svm_train_accuracy:.3f}')\n",
    "print(f'测试集准确率: {svm_test_accuracy:.3f}\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################# 绘制损失曲线和准确率对比\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# 画 HQNN 损失曲线\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='tab:red')\n",
    "ax1.plot(range(1, len(losses) + 1), losses, color='tab:red', label=\"HQNN Loss\")\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# 共享 x 轴，画 HQNN 训练准确率 vs SVM\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy', color='tab:blue')\n",
    "ax2.plot(range(1, len(train_accuracies) + 1), train_accuracies, color='tab:blue', linestyle='dashed',\n",
    "         label=f\"HQNN Train Accuracy:{train_accuracies[-1]:.3f}\")\n",
    "ax2.plot(range(1, len(test_accuracies) + 1), test_accuracies, color='tab:orange', linestyle='dashed',\n",
    "         label=f\"HQNN test Accuracy:{test_accuracies[-1]:.3f}\")\n",
    "\n",
    "ax2.axhline(y=svm_train_accuracy, color='green', linestyle='--', label=f\"SVM Train Accuracy:{svm_train_accuracy:.3f}\")\n",
    "ax2.axhline(y=svm_test_accuracy, color='purple', linestyle='--', label=f\"SVM Test Accuracy:{svm_test_accuracy:.3f}\")\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "\n",
    "# 标题包含特征数目\n",
    "feature_count = X_train_tensor.shape[1]\n",
    "plt.title(f\"Training Convergence and Accuracy Comparison (Feature Count: {feature_count})\")\n",
    "\n",
    "fig.legend(loc=\"center right\")\n",
    "plt.savefig(\"hqnn_svm_comparison4_6q_batch.pdf\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################画图\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D 绘图需要\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data = pd.read_csv('misclassified_samples_cnn.txt')\n",
    "# CSV 文件的列分别为 \"特征\", \"真实标签\", \"预测标签\"\n",
    "# 其中 \"特征\" 字段中，各个特征以空格分隔\n",
    "\n",
    "# 解析特征，转换为数值列表\n",
    "features = data['特征'].apply(lambda s: [float(x) for x in s.split()]).tolist()\n",
    "features = np.array(features)  # shape: (n_samples, n_features)\n",
    "true_labels = data['真实标签'].values\n",
    "predicted_labels = data['预测标签'].values\n",
    "\n",
    "n_samples, n_features = features.shape\n",
    "\n",
    "# 构造新的数据矩阵，每一行依次包含：特征1, 特征2, ..., 特征n, 真实标签, 预测标签\n",
    "data_matrix = np.hstack([features, true_labels.reshape(-1, 1), predicted_labels.reshape(-1, 1)])\n",
    "\n",
    "# x 轴对应的标签名称：前 n_features 个为特征名称，后面两个分别为真实标签和预测标签\n",
    "x_labels = [f'F{i+1}' for i in range(n_features)] + ['True', 'Pred']\n",
    "x = np.arange(len(x_labels))  # 数值化 x 轴\n",
    "\n",
    "# 创建 3D 图形\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 对于每个样本，在 x 轴上对应每个特征/标签的位置上绘制其数值，y 轴为样本序号\n",
    "for i in range(n_samples):\n",
    "    # 使用 plot 连接每个样本在各个位置的数值点，也可以用 scatter 单独绘制每个点\n",
    "    ax.plot(x, [i] * len(x), data_matrix[i, :], marker='o', label=f\"样本 {i}\" if i==0 else \"\")\n",
    "    # 如果希望样本之间颜色不同，也可以加入颜色控制\n",
    "\n",
    "# 设置 x 轴为类别标签\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "ax.set_xlabel('特征 / 标签')\n",
    "ax.set_ylabel('样本序号')\n",
    "ax.set_zlabel('值')\n",
    "ax.set_title('预测错误样本的特征及标签可视化')\n",
    "plt.savefig(\"features.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d757198-ab8a-49ed-b4ee-fb00d1268965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Data...\n",
      "Global Unitary Mode: 2 circuits, 8x8 each.\n",
      "Total Quantum Params: 256\n",
      "PQNN 参数量: 346 (Trainable: 328)\n",
      ">>> Epoch 1: Freezing fixed_matrix for Pure Quantum Training...\n",
      "Epoch 1/500 | Loss: 0.3426 | Train: 90.35% | Test: 89.78%\n",
      "Epoch 5/500 | Loss: 0.1981 | Train: 94.59% | Test: 95.59%\n",
      "Epoch 10/500 | Loss: 0.1397 | Train: 96.91% | Test: 96.85%\n",
      "Epoch 15/500 | Loss: 0.1291 | Train: 97.09% | Test: 96.71%\n",
      "Epoch 20/500 | Loss: 0.1238 | Train: 97.18% | Test: 96.85%\n",
      "Epoch 25/500 | Loss: 0.1186 | Train: 97.21% | Test: 96.71%\n",
      "Epoch 30/500 | Loss: 0.1161 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 35/500 | Loss: 0.1147 | Train: 97.21% | Test: 96.71%\n",
      "Epoch 40/500 | Loss: 0.1155 | Train: 97.12% | Test: 96.78%\n",
      "Epoch 45/500 | Loss: 0.1215 | Train: 97.06% | Test: 96.71%\n",
      "Epoch 50/500 | Loss: 0.1139 | Train: 97.18% | Test: 96.78%\n",
      "Epoch 55/500 | Loss: 0.1136 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 60/500 | Loss: 0.1127 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 65/500 | Loss: 0.1124 | Train: 97.24% | Test: 96.64%\n",
      "Epoch 70/500 | Loss: 0.1113 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 75/500 | Loss: 0.1114 | Train: 97.30% | Test: 96.64%\n",
      "Epoch 80/500 | Loss: 0.1106 | Train: 97.24% | Test: 96.64%\n",
      "Epoch 85/500 | Loss: 0.1108 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 90/500 | Loss: 0.1101 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 95/500 | Loss: 0.1103 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 100/500 | Loss: 0.1097 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 105/500 | Loss: 0.1099 | Train: 97.30% | Test: 96.78%\n",
      "Epoch 110/500 | Loss: 0.1093 | Train: 97.21% | Test: 96.64%\n",
      "Epoch 115/500 | Loss: 0.1095 | Train: 97.27% | Test: 96.78%\n",
      "Epoch 120/500 | Loss: 0.1090 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 125/500 | Loss: 0.1091 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 130/500 | Loss: 0.1087 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 135/500 | Loss: 0.1087 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 140/500 | Loss: 0.1083 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 145/500 | Loss: 0.1082 | Train: 97.21% | Test: 96.71%\n",
      "Epoch 150/500 | Loss: 0.1080 | Train: 97.27% | Test: 96.78%\n",
      "Epoch 155/500 | Loss: 0.1079 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 160/500 | Loss: 0.1078 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 165/500 | Loss: 0.1078 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 170/500 | Loss: 0.1077 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 175/500 | Loss: 0.1076 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 180/500 | Loss: 0.1075 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 185/500 | Loss: 0.1075 | Train: 97.24% | Test: 96.78%\n",
      "Epoch 190/500 | Loss: 0.1074 | Train: 97.21% | Test: 96.78%\n",
      "Epoch 195/500 | Loss: 0.1073 | Train: 97.21% | Test: 96.71%\n",
      "Epoch 200/500 | Loss: 0.1073 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 205/500 | Loss: 0.1072 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 210/500 | Loss: 0.1071 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 215/500 | Loss: 0.1071 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 220/500 | Loss: 0.1070 | Train: 97.24% | Test: 96.64%\n",
      "Epoch 225/500 | Loss: 0.1070 | Train: 97.24% | Test: 96.64%\n",
      "Epoch 230/500 | Loss: 0.1069 | Train: 97.24% | Test: 96.64%\n",
      "Epoch 235/500 | Loss: 0.1069 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 240/500 | Loss: 0.1068 | Train: 97.24% | Test: 96.71%\n",
      "Epoch 245/500 | Loss: 0.1068 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 250/500 | Loss: 0.1067 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 255/500 | Loss: 0.1067 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 260/500 | Loss: 0.1066 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 265/500 | Loss: 0.1066 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 270/500 | Loss: 0.1066 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 275/500 | Loss: 0.1065 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 280/500 | Loss: 0.1065 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 285/500 | Loss: 0.1064 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 290/500 | Loss: 0.1064 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 295/500 | Loss: 0.1064 | Train: 97.27% | Test: 96.71%\n",
      "Epoch 300/500 | Loss: 0.1063 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 305/500 | Loss: 0.1063 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 310/500 | Loss: 0.1063 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 315/500 | Loss: 0.1063 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 320/500 | Loss: 0.1063 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 325/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 330/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 335/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 340/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 345/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 350/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 355/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 360/500 | Loss: 0.1062 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 365/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 370/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 375/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 380/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 385/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 390/500 | Loss: 0.1061 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 395/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 400/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 405/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 410/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 415/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 420/500 | Loss: 0.1060 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 425/500 | Loss: 0.1059 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 430/500 | Loss: 0.1059 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 435/500 | Loss: 0.1059 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 440/500 | Loss: 0.1059 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 445/500 | Loss: 0.1059 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 450/500 | Loss: 0.1058 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 455/500 | Loss: 0.1058 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 460/500 | Loss: 0.1058 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 465/500 | Loss: 0.1058 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 470/500 | Loss: 0.1058 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 475/500 | Loss: 0.1057 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 480/500 | Loss: 0.1057 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 485/500 | Loss: 0.1057 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 490/500 | Loss: 0.1057 | Train: 97.30% | Test: 96.71%\n",
      "Epoch 495/500 | Loss: 0.1056 | Train: 97.33% | Test: 96.71%\n",
      "Epoch 500/500 | Loss: 0.1057 | Train: 97.30% | Test: 96.71%\n",
      ">>> PQNN 训练日志已保存至: log_pqnn.npy\n",
      "Saved 47 misclassified samples.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from myfun_QNN_fast import r_quantum_layer_fast\n",
    "\n",
    "# 检测设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===========================\n",
    "# 1. 数据加载 (全程 float64)\n",
    "# ===========================\n",
    "print(\"Loading Data...\")\n",
    "file_path = r'./data/data_koopman_6.txt'\n",
    "try:\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "except:\n",
    "    # 兼容测试：如果没有数据文件，生成随机数据\n",
    "    print(\"Warning: Data file not found, using random data.\")\n",
    "    data = np.random.rand(100, 7)\n",
    "    data[:, -1] = np.random.randint(0, 2, 100)\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# 标签处理\n",
    "unique_labels = np.unique(y)\n",
    "label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 【关键】全程使用 float64 (double) 以匹配量子精度\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float64).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float64).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# ===========================\n",
    "# 2. 模型定义\n",
    "# ===========================\n",
    "\n",
    "class HQNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(HQNN, self).__init__()\n",
    "        \n",
    "        # 【新增】1. 批归一化层 (Batch Normalization)\n",
    "        # 在数据进入第一层矩阵前，先做一次动态归一化\n",
    "        self.bn_input = nn.BatchNorm1d(num_features).double()\n",
    "\n",
    "        # 2. 投影层 (初始设为可训练)\n",
    "        # bias=False 是因为 BN 层已经有了偏置项\n",
    "        self.fixed_matrix = nn.Linear(num_features, 6, bias=False).double()\n",
    "\n",
    "        # self.bn_input2 = nn.BatchNorm1d(6).double()\n",
    "\n",
    "        self.ln_input = nn.LayerNorm(num_features).double()\n",
    "        \n",
    "        # 假设量子层输出维度等于量子比特数\n",
    "        self.ln_output = nn.LayerNorm(6).double()\n",
    "        \n",
    "\n",
    "        # 【修改】不要在这里冻结它！我们要让它先练 10 轮。\n",
    "        # for param in self.fixed_matrix.parameters():\n",
    "        #     param.requires_grad = False\n",
    "            \n",
    "        # 3. 量子参数 (Global Unitary)\n",
    "        split_re = 2               \n",
    "        num_sub_qubits = 6 // split_re \n",
    "        dim = 2 ** num_sub_qubits      \n",
    "        \n",
    "        params_per_circuit = 2 * (dim * dim) \n",
    "        total_q_params = params_per_circuit * split_re\n",
    "        \n",
    "        print(f\"Global Unitary Mode: {split_re} circuits, {dim}x{dim} each.\")\n",
    "        print(f\"Total Quantum Params: {total_q_params}\")\n",
    "\n",
    "        # 初始化量子参数\n",
    "        self.q_params = nn.Parameter(torch.randn(total_q_params, 1, dtype=torch.float64) * 0.1)\n",
    "        \n",
    "        # 4. 后处理 (保持冻结)\n",
    "        self.fc3 = nn.Linear(6, 2)\n",
    "        self.bn3 = nn.BatchNorm1d(2)\n",
    "        for param in self.fc3.parameters(): param.requires_grad = False\n",
    "        for param in self.bn3.parameters(): param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入精度匹配\n",
    "        x = x.to(dtype=torch.float64)\n",
    "        \n",
    "        # 【新增】先通过 BN 层归一化\n",
    "        x = self.ln_input(x)\n",
    "        \n",
    "        # 通过投影层 (前  轮可训练，之后冻结)\n",
    "        x = self.fixed_matrix(x) \n",
    "        \n",
    "        # 量子层\n",
    "        q_out = r_quantum_layer_fast(2, self.q_params, x)      \n",
    "        # q_out = self.bn_input2(q_out)\n",
    "        q_out = self.ln_output(q_out)\n",
    "        \n",
    "        # 测量映射\n",
    "        out0 = q_out[:, ::2].mean(dim=1)\n",
    "        out1 = q_out[:, 1::2].mean(dim=1)\n",
    "        out = torch.stack([out0, out1], dim=1).to(dtype=torch.float32)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ===========================\n",
    "# 3. 训练配置\n",
    "# ===========================\n",
    "model = HQNN(num_features=6).to(device)\n",
    "\n",
    "# 【关键修复】学习率从 0.1 降为 0.01，防止震荡\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ===========================\n",
    "# 4. 训练循环\n",
    "# ===========================\n",
    "def train_model(model, X_train, y_train, epochs=50, batch_size=32):\n",
    "    # --- 【新增 1】初始化日志 ---\n",
    "    # 计算参数量 (注意：这里计算的是总参数量，或者你可以只计算 requires_grad 的)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    log_data = {\n",
    "        'name': 'PQNN (Koopman 6D)',\n",
    "        'params': total_params, # 或者 trainable_params，看你想展示哪个\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    print(f\"PQNN 参数量: {total_params} (Trainable: {trainable_params})\")\n",
    "    # ---------------------------\n",
    "\n",
    "    model.train()\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # 策略：第1轮冻结经典层 (模拟从一开始就只有量子训练)\n",
    "        if epoch == 0:\n",
    "            print(f\">>> Epoch {epoch+1}: Freezing fixed_matrix for Pure Quantum Training...\")\n",
    "            for param in model.fixed_matrix.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 统计 Loss 和 训练精度\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "\n",
    "        # 计算本轮指标\n",
    "        avg_loss = running_loss / num_batches\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # --- 计算测试集精度 (每轮都算，为了画平滑曲线) ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs_test = model(X_test_tensor)\n",
    "            _, preds_test = torch.max(outputs_test, 1)\n",
    "            test_acc = accuracy_score(y_test_tensor.cpu().numpy(), preds_test.cpu().numpy()) * 100\n",
    "        model.train()\n",
    "        \n",
    "        # --- 【新增 2】记录数据 ---\n",
    "        log_data['loss'].append(avg_loss)\n",
    "        log_data['train_acc'].append(train_acc)\n",
    "        log_data['test_acc'].append(test_acc)\n",
    "        log_data['epochs'].append(epoch + 1)\n",
    "        \n",
    "        # 打印 (减少刷屏，每5轮打印一次，但数据每轮都存)\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
    "\n",
    "    # --- 【新增 3】保存日志 ---\n",
    "    np.save('log_pqnn.npy', log_data)\n",
    "    print(\">>> PQNN 训练日志已保存至: log_pqnn.npy\")\n",
    "    \n",
    "    return preds_test.cpu().numpy()\n",
    "\n",
    "# 运行训练\n",
    "final_preds = train_model(model, X_train_tensor, y_train_tensor, epochs=500, batch_size=64)\n",
    "\n",
    "# 保存误分类样本 (保持原样)\n",
    "y_test_cpu = y_test_tensor.cpu().numpy()\n",
    "misclassified_indices = np.where(final_preds != y_test_cpu)[0]\n",
    "with open('misclassified_samples_cnn.txt', 'w') as f:\n",
    "    f.write(\"Feature,True,Pred\\n\")\n",
    "    for idx in misclassified_indices:\n",
    "        feat_str = \" \".join(map(str, X_test_tensor[idx].cpu().numpy()))\n",
    "        f.write(f\"{feat_str},{y_test_cpu[idx]},{final_preds[idx]}\\n\")\n",
    "print(f\"Saved {len(misclassified_indices)} misclassified samples.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3590de28-4076-41ee-81cc-5d43de8d9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Koopman Data from ./data/data_koopman_6.txt...\n",
      "Global Unitary Mode: 2 circuits, 8x8 each.\n",
      "Total Quantum Params: 256\n",
      "\n",
      ">>> Starting Training...\n",
      "Epoch 001 | L1: Trainable | Loss: 0.1529 | Train Acc: 0.969 | Test Acc: 0.964\n",
      "Epoch 010 | L1: Trainable | Loss: 0.0790 | Train Acc: 0.984 | Test Acc: 0.964\n",
      ">>> [Strategy] Epoch 10: Freezing Projection Layer (fixed_matrix) now!\n",
      "Epoch 020 | L1: Frozen    | Loss: 0.0833 | Train Acc: 0.969 | Test Acc: 0.967\n",
      "Epoch 030 | L1: Frozen    | Loss: 0.0859 | Train Acc: 0.969 | Test Acc: 0.968\n",
      "Epoch 040 | L1: Frozen    | Loss: 0.0876 | Train Acc: 0.969 | Test Acc: 0.967\n",
      "Epoch 050 | L1: Frozen    | Loss: 0.0937 | Train Acc: 0.969 | Test Acc: 0.967\n",
      "Epoch 060 | L1: Frozen    | Loss: 0.0954 | Train Acc: 0.969 | Test Acc: 0.969\n",
      "Epoch 070 | L1: Frozen    | Loss: 0.0804 | Train Acc: 0.984 | Test Acc: 0.966\n",
      "Epoch 080 | L1: Frozen    | Loss: 0.0867 | Train Acc: 0.984 | Test Acc: 0.967\n",
      "Epoch 090 | L1: Frozen    | Loss: 0.0903 | Train Acc: 0.969 | Test Acc: 0.967\n",
      "Epoch 100 | L1: Frozen    | Loss: 0.0908 | Train Acc: 0.969 | Test Acc: 0.968\n",
      ">>> Training Finished.\n",
      "\n",
      ">>> Starting Feature Visualization Pipeline...\n",
      "(1/3) Loading Raw Data from ./data/raw_dataset_standardized.npy...\n",
      "      Processing sequences (padding & downsampling)...\n",
      "(2/3) Extracting Koopman Features...\n",
      "(3/3) Extracting Quantum Latent Features...\n",
      "\n",
      ">>> Running t-SNE and plotting...\n",
      ">>> Figure saved successfully to 'Figure5_Evolution.pdf'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # 服务器端绘图，不显示窗口\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# 引入您的快速量子层函数\n",
    "try:\n",
    "    from myfun_QNN_fast import r_quantum_layer_fast\n",
    "except ImportError:\n",
    "    print(\"【错误】未找到 myfun_QNN_fast.py，请确保该文件在当前目录下。\")\n",
    "\n",
    "# 配置\n",
    "CONFIG = {\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'raw_data_path': r'./data/raw_dataset_standardized.npy',\n",
    "    'koopman_data_path': r'./data/data_koopman_6.txt',\n",
    "    'seed': 42,\n",
    "    'epochs': 100,          # 总训练轮数\n",
    "    'freeze_epoch': 10,    # 第几轮冻结第一层\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.01\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: 模型定义与训练流程 (Training Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class HQNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(HQNN, self).__init__()\n",
    "        \n",
    "        # 【新增】1. 批归一化层 (Batch Normalization)\n",
    "        # 在数据进入第一层矩阵前，先做一次动态归一化\n",
    "        self.bn_input = nn.BatchNorm1d(num_features).double()\n",
    "\n",
    "        # 2. 投影层 (初始设为可训练)\n",
    "        # bias=False 是因为 BN 层已经有了偏置项\n",
    "        self.fixed_matrix = nn.Linear(num_features, 6, bias=False).double()\n",
    "\n",
    "        # self.bn_input2 = nn.BatchNorm1d(6).double()\n",
    "\n",
    "        self.ln_input = nn.LayerNorm(num_features).double()\n",
    "        \n",
    "        # 假设量子层输出维度等于量子比特数\n",
    "        self.ln_output = nn.LayerNorm(6).double()\n",
    "        \n",
    "\n",
    "        # 【修改】不要在这里冻结它！我们要让它先练 10 轮。\n",
    "        # for param in self.fixed_matrix.parameters():\n",
    "        #     param.requires_grad = False\n",
    "            \n",
    "        # 3. 量子参数 (Global Unitary)\n",
    "        split_re = 2               \n",
    "        num_sub_qubits = 6 // split_re \n",
    "        dim = 2 ** num_sub_qubits      \n",
    "        \n",
    "        params_per_circuit = 2 * (dim * dim) \n",
    "        total_q_params = params_per_circuit * split_re\n",
    "        \n",
    "        print(f\"Global Unitary Mode: {split_re} circuits, {dim}x{dim} each.\")\n",
    "        print(f\"Total Quantum Params: {total_q_params}\")\n",
    "\n",
    "        # 初始化量子参数\n",
    "        self.q_params = nn.Parameter(torch.randn(total_q_params, 1, dtype=torch.float64) * 0.1)\n",
    "        \n",
    "        # 4. 后处理 (保持冻结)\n",
    "        self.fc3 = nn.Linear(6, 2)\n",
    "        self.bn3 = nn.BatchNorm1d(2)\n",
    "        for param in self.fc3.parameters(): param.requires_grad = False\n",
    "        for param in self.bn3.parameters(): param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入精度匹配\n",
    "        x = x.to(dtype=torch.float64)\n",
    "        \n",
    "        # 【新增】先通过 BN 层归一化\n",
    "        x = self.ln_input(x)\n",
    "        \n",
    "        # 通过投影层 (前  轮可训练，之后冻结)\n",
    "        x = self.fixed_matrix(x) \n",
    "        \n",
    "        # 量子层\n",
    "        q_out = r_quantum_layer_fast(2, self.q_params, x)      \n",
    "        q_out = self.ln_output(q_out)\n",
    "        \n",
    "        # 测量映射\n",
    "        out0 = q_out[:, ::2].mean(dim=1)\n",
    "        out1 = q_out[:, 1::2].mean(dim=1)\n",
    "        out = torch.stack([out0, out1], dim=1).to(dtype=torch.float32)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "def load_and_process_data():\n",
    "    print(f\"Loading Koopman Data from {CONFIG['koopman_data_path']}...\")\n",
    "    try:\n",
    "        data = np.loadtxt(CONFIG['koopman_data_path'], delimiter=',')\n",
    "    except:\n",
    "        print(\"Warning: Data file not found, creating random mock data.\")\n",
    "        data = np.random.rand(100, 7)\n",
    "        data[:, -1] = np.random.randint(0, 2, 100)\n",
    "\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # 标签映射\n",
    "    unique_labels = np.unique(y)\n",
    "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "    y_mapped = np.vectorize(label_map.get)(y)\n",
    "\n",
    "    # 划分数据集 (Random State 必须固定为 42，以便后续与 Raw Data 对齐)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.3, random_state=CONFIG['seed'])\n",
    "\n",
    "    # 归一化\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 转 Tensor (Double Precision)\n",
    "    device = CONFIG['device']\n",
    "    data_dict = {\n",
    "        'X_train': torch.tensor(X_train_scaled, dtype=torch.float64).to(device),\n",
    "        'X_test': torch.tensor(X_test_scaled, dtype=torch.float64).to(device),\n",
    "        'y_train': torch.tensor(y_train, dtype=torch.long).to(device),\n",
    "        'y_test_cpu': y_test, # 用于后续绘图\n",
    "        'y_test_tensor': torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "def train_pipeline(model, data_dict):\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test_tensor']\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // CONFIG['batch_size']\n",
    "    \n",
    "    print(\"\\n>>> Starting Training...\")\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # --- 策略：第 10 轮冻结第一层 ---\n",
    "        if epoch == CONFIG['freeze_epoch']:\n",
    "            print(f\">>> [Strategy] Epoch {epoch}: Freezing Projection Layer (fixed_matrix) now!\")\n",
    "            for param in model.fixed_matrix.parameters():\n",
    "                param.requires_grad = False\n",
    "            # 可选：微调阶段降低学习率\n",
    "            # for g in optimizer.param_groups: g['lr'] *= 0.1 \n",
    "\n",
    "        model.train()\n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * CONFIG['batch_size']\n",
    "            end = start + CONFIG['batch_size']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train[start:end])\n",
    "            loss = criterion(outputs, y_train[start:end])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 评估\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Train Acc\n",
    "                _, preds_tr = torch.max(outputs, 1)\n",
    "                train_acc = accuracy_score(y_train[start:end].cpu().numpy(), preds_tr.cpu().numpy())\n",
    "                # Test Acc\n",
    "                out_test = model(X_test)\n",
    "                _, preds_te = torch.max(out_test, 1)\n",
    "                test_acc = accuracy_score(y_test.cpu().numpy(), preds_te.cpu().numpy())\n",
    "            \n",
    "            status = \"Frozen\" if not model.fixed_matrix.weight.requires_grad else \"Trainable\"\n",
    "            print(f\"Epoch {epoch+1:03d} | L1: {status:<9} | Loss: {loss.item():.4f} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")\n",
    "\n",
    "    print(\">>> Training Finished.\")\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: 可视化流程 (Visualization Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def pad_sequences(sequences, downsample=10):\n",
    "    \"\"\"辅助函数：将变长序列填充为矩阵，并降采样加速 t-SNE\"\"\"\n",
    "    # 简单降采样\n",
    "    seqs_ds = [s[::downsample] for s in sequences]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    # 填充\n",
    "    matrix = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        matrix[i, :len(s)] = s\n",
    "    return matrix\n",
    "\n",
    "def visualize_feature_evolution(model, data_dict):\n",
    "    \"\"\"\n",
    "    绘制图 5：Raw Data -> Koopman -> Quantum Latent 的演化\n",
    "    \"\"\"\n",
    "    print(\"\\n>>> Starting Feature Visualization Pipeline...\")\n",
    "    \n",
    "    # 1. 准备标签和颜色\n",
    "    y_test = data_dict['y_test_cpu']\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # 阶段 A: 获取原始数据 (Raw Data)\n",
    "    # -------------------------------------------------\n",
    "    print(f\"(1/3) Loading Raw Data from {CONFIG['raw_data_path']}...\")\n",
    "    try:\n",
    "        raw_dict = np.load(CONFIG['raw_data_path'], allow_pickle=True).item()\n",
    "        raw_seqs = raw_dict['sequences']\n",
    "        raw_labels = raw_dict['labels']\n",
    "        \n",
    "        # 关键：必须执行完全相同的 split 以对齐数据\n",
    "        _, X_test_raw_seq, _, _ = train_test_split(raw_seqs, raw_labels, test_size=0.3, random_state=CONFIG['seed'])\n",
    "        \n",
    "        # 处理变长序列 -> 矩阵\n",
    "        print(\"      Processing sequences (padding & downsampling)...\")\n",
    "        X_raw_vis = pad_sequences(X_test_raw_seq, downsample=20) # 20倍降采样\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      [Error] Could not load raw data: {e}\")\n",
    "        print(\"      Skipping Raw Data visualization (creating dummy noise for layout).\")\n",
    "        X_raw_vis = np.random.randn(len(y_test), 50) # Dummy\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 阶段 B: 获取 Koopman 特征 (QNN 输入)\n",
    "    # -------------------------------------------------\n",
    "    print(\"(2/3) Extracting Koopman Features...\")\n",
    "    X_test_tensor = data_dict['X_test']\n",
    "    X_koopman_vis = X_test_tensor.cpu().numpy()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 阶段 C: 获取量子潜层特征 (Quantum Latent)\n",
    "    # -------------------------------------------------\n",
    "    print(\"(3/3) Extracting Quantum Latent Features...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = X_test_tensor.to(dtype=torch.float64)\n",
    "        # x = model.bn_input(x)\n",
    "        x = model.ln_input(x)\n",
    "        x = model.fixed_matrix(x)\n",
    "        # 获取 Quantum Layer 的输出 (在 Mean 之前)\n",
    "        q_out = r_quantum_layer_fast(2, model.q_params, x)\n",
    "        q_out = model.ln_output(q_out)\n",
    "        \n",
    "        out0 = q_out[:, ::2].mean(dim=1)\n",
    "        out1 = q_out[:, 1::2].mean(dim=1)\n",
    "        out = torch.stack([out0, out1], dim=1).to(dtype=torch.float32)\n",
    "\n",
    "        X_quantum_vis = out.cpu().numpy()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 绘图逻辑 (Three Panels)\n",
    "    # -------------------------------------------------\n",
    "    data_map = [\n",
    "        ('Raw Data Space', X_raw_vis),\n",
    "        ('Koopman Features', X_koopman_vis),\n",
    "        ('Quantum Latent Space', X_quantum_vis)\n",
    "    ]\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue, Orange\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"\\n>>> Running t-SNE and plotting...\")\n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 降维\n",
    "        # tsne = TSNE(n_components=2, perplexity=50, n_iter=2000, random_state=42)\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # 计算可分性指标 (Silhouette Score)\n",
    "        try:\n",
    "            score = silhouette_score(data, y_test)\n",
    "        except: score = 0\n",
    "        \n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.6, s=20, edgecolors='none')\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )            \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # 添加指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 全局图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.05), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Figure5_Evolution.pdf', dpi=300, bbox_inches='tight')\n",
    "    print(f\">>> Figure saved successfully to 'Figure5_Evolution.pdf'\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {CONFIG['device']}\")\n",
    "    \n",
    "    # 1. 准备数据\n",
    "    data = load_and_process_data()\n",
    "    \n",
    "    # 2. 初始化模型\n",
    "    model = HQNN(num_features=6).to(CONFIG['device'])\n",
    "    \n",
    "    # 3. 运行训练 (Part 1)\n",
    "    trained_model = train_pipeline(model, data)\n",
    "\n",
    "    # 4. 运行可视化 (Part 2)\n",
    "    visualize_feature_evolution(trained_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efb1e5-d30e-4ca4-af41-4a2e84551acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20e7280-77dc-4594-9fcb-b4751d7efd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载数据: ./data/data_koopman_sequence.txt ...\n",
      "总样本数: 4763\n",
      "模型参数量: 9,250\n",
      "开始训练 KoopmanCNN...\n",
      "Epoch [1/500] | Loss: 0.4744 | Train: 94.48% | Test: 92.65%\n",
      "Epoch [2/500] | Loss: 0.3135 | Train: 95.20% | Test: 92.09%\n",
      "Epoch [3/500] | Loss: 0.2231 | Train: 95.23% | Test: 92.44%\n",
      "Epoch [4/500] | Loss: 0.1827 | Train: 95.38% | Test: 92.09%\n",
      "Epoch [5/500] | Loss: 0.1668 | Train: 95.95% | Test: 93.28%\n",
      "Epoch [6/500] | Loss: 0.1644 | Train: 96.13% | Test: 94.19%\n",
      "Epoch [7/500] | Loss: 0.1530 | Train: 96.22% | Test: 94.47%\n",
      "Epoch [8/500] | Loss: 0.1468 | Train: 96.28% | Test: 93.77%\n",
      "Epoch [9/500] | Loss: 0.1426 | Train: 96.46% | Test: 94.33%\n",
      "Epoch [10/500] | Loss: 0.1385 | Train: 96.64% | Test: 94.26%\n",
      "Epoch [11/500] | Loss: 0.1392 | Train: 96.40% | Test: 94.40%\n",
      "Epoch [12/500] | Loss: 0.1336 | Train: 96.64% | Test: 92.65%\n",
      "Epoch [13/500] | Loss: 0.1350 | Train: 96.70% | Test: 94.26%\n",
      "Epoch [14/500] | Loss: 0.1306 | Train: 96.73% | Test: 94.19%\n",
      "Epoch [15/500] | Loss: 0.1288 | Train: 96.79% | Test: 95.24%\n",
      "Epoch [16/500] | Loss: 0.1311 | Train: 96.67% | Test: 94.75%\n",
      "Epoch [17/500] | Loss: 0.1281 | Train: 96.70% | Test: 95.38%\n",
      "Epoch [18/500] | Loss: 0.1284 | Train: 96.97% | Test: 94.68%\n",
      "Epoch [19/500] | Loss: 0.1262 | Train: 96.82% | Test: 94.82%\n",
      "Epoch [20/500] | Loss: 0.1219 | Train: 97.09% | Test: 95.10%\n",
      "Epoch [21/500] | Loss: 0.1227 | Train: 96.94% | Test: 94.89%\n",
      "Epoch [22/500] | Loss: 0.1229 | Train: 97.18% | Test: 94.05%\n",
      "Epoch [23/500] | Loss: 0.1255 | Train: 96.79% | Test: 95.10%\n",
      "Epoch [24/500] | Loss: 0.1198 | Train: 96.82% | Test: 94.96%\n",
      "Epoch [25/500] | Loss: 0.1235 | Train: 96.94% | Test: 94.96%\n",
      "Epoch [26/500] | Loss: 0.1201 | Train: 97.06% | Test: 95.87%\n",
      "Epoch [27/500] | Loss: 0.1202 | Train: 96.94% | Test: 95.80%\n",
      "Epoch [28/500] | Loss: 0.1176 | Train: 96.94% | Test: 94.96%\n",
      "Epoch [29/500] | Loss: 0.1153 | Train: 97.00% | Test: 95.80%\n",
      "Epoch [30/500] | Loss: 0.1111 | Train: 97.54% | Test: 95.31%\n",
      "Epoch [31/500] | Loss: 0.1177 | Train: 97.09% | Test: 96.08%\n",
      "Epoch [32/500] | Loss: 0.1159 | Train: 96.97% | Test: 96.01%\n",
      "Epoch [33/500] | Loss: 0.1178 | Train: 96.94% | Test: 95.10%\n",
      "Epoch [34/500] | Loss: 0.1142 | Train: 97.27% | Test: 96.01%\n",
      "Epoch [35/500] | Loss: 0.1176 | Train: 97.03% | Test: 95.17%\n",
      "Epoch [36/500] | Loss: 0.1061 | Train: 97.39% | Test: 95.87%\n",
      "Epoch [37/500] | Loss: 0.1135 | Train: 97.09% | Test: 96.01%\n",
      "Epoch [38/500] | Loss: 0.1137 | Train: 97.24% | Test: 95.73%\n",
      "Epoch [39/500] | Loss: 0.1148 | Train: 97.18% | Test: 95.94%\n",
      "Epoch [40/500] | Loss: 0.1136 | Train: 97.18% | Test: 95.94%\n",
      "Epoch [41/500] | Loss: 0.1111 | Train: 97.42% | Test: 96.29%\n",
      "Epoch [42/500] | Loss: 0.1069 | Train: 97.48% | Test: 96.29%\n",
      "Epoch [43/500] | Loss: 0.1089 | Train: 97.39% | Test: 95.94%\n",
      "Epoch [44/500] | Loss: 0.1115 | Train: 97.12% | Test: 96.43%\n",
      "Epoch [45/500] | Loss: 0.1065 | Train: 97.39% | Test: 96.29%\n",
      "Epoch [46/500] | Loss: 0.1084 | Train: 97.30% | Test: 96.36%\n",
      "Epoch [47/500] | Loss: 0.1051 | Train: 97.54% | Test: 96.22%\n",
      "Epoch [48/500] | Loss: 0.1059 | Train: 97.33% | Test: 96.36%\n",
      "Epoch [49/500] | Loss: 0.1072 | Train: 97.36% | Test: 96.15%\n",
      "Epoch [50/500] | Loss: 0.1068 | Train: 97.33% | Test: 96.57%\n",
      "Epoch [51/500] | Loss: 0.1076 | Train: 97.27% | Test: 96.36%\n",
      "Epoch [52/500] | Loss: 0.1053 | Train: 97.39% | Test: 96.50%\n",
      "Epoch [53/500] | Loss: 0.1031 | Train: 97.48% | Test: 96.64%\n",
      "Epoch [54/500] | Loss: 0.1038 | Train: 97.30% | Test: 96.08%\n",
      "Epoch [55/500] | Loss: 0.1055 | Train: 97.39% | Test: 96.36%\n",
      "Epoch [56/500] | Loss: 0.1054 | Train: 97.30% | Test: 96.36%\n",
      "Epoch [57/500] | Loss: 0.1064 | Train: 97.45% | Test: 96.57%\n",
      "Epoch [58/500] | Loss: 0.1042 | Train: 97.45% | Test: 96.64%\n",
      "Epoch [59/500] | Loss: 0.1035 | Train: 97.45% | Test: 96.29%\n",
      "Epoch [60/500] | Loss: 0.1014 | Train: 97.54% | Test: 96.36%\n",
      "Epoch [61/500] | Loss: 0.1006 | Train: 97.51% | Test: 96.43%\n",
      "Epoch [62/500] | Loss: 0.1090 | Train: 97.30% | Test: 96.29%\n",
      "Epoch [63/500] | Loss: 0.1113 | Train: 97.33% | Test: 96.01%\n",
      "Epoch [64/500] | Loss: 0.1011 | Train: 97.42% | Test: 96.43%\n",
      "Epoch [65/500] | Loss: 0.1028 | Train: 97.51% | Test: 96.64%\n",
      "Epoch [66/500] | Loss: 0.1084 | Train: 97.33% | Test: 96.43%\n",
      "Epoch [67/500] | Loss: 0.1008 | Train: 97.54% | Test: 95.52%\n",
      "Epoch [68/500] | Loss: 0.0946 | Train: 97.60% | Test: 96.78%\n",
      "Epoch [69/500] | Loss: 0.1042 | Train: 97.24% | Test: 96.15%\n",
      "Epoch [70/500] | Loss: 0.0958 | Train: 97.48% | Test: 96.50%\n",
      "Epoch [71/500] | Loss: 0.1018 | Train: 97.21% | Test: 96.92%\n",
      "Epoch [72/500] | Loss: 0.1070 | Train: 97.21% | Test: 96.22%\n",
      "Epoch [73/500] | Loss: 0.0984 | Train: 97.51% | Test: 96.08%\n",
      "Epoch [74/500] | Loss: 0.1013 | Train: 97.48% | Test: 96.78%\n",
      "Epoch [75/500] | Loss: 0.1034 | Train: 97.54% | Test: 95.80%\n",
      "Epoch [76/500] | Loss: 0.1021 | Train: 97.45% | Test: 96.71%\n",
      "Epoch [77/500] | Loss: 0.1013 | Train: 97.69% | Test: 96.50%\n",
      "Epoch [78/500] | Loss: 0.1014 | Train: 97.54% | Test: 96.78%\n",
      "Epoch [79/500] | Loss: 0.0969 | Train: 97.63% | Test: 96.92%\n",
      "Epoch [80/500] | Loss: 0.1037 | Train: 97.54% | Test: 96.78%\n",
      "Epoch [81/500] | Loss: 0.1026 | Train: 97.54% | Test: 96.29%\n",
      "Epoch [82/500] | Loss: 0.0992 | Train: 97.48% | Test: 96.64%\n",
      "Epoch [83/500] | Loss: 0.1025 | Train: 97.39% | Test: 95.87%\n",
      "Epoch [84/500] | Loss: 0.0975 | Train: 97.66% | Test: 96.01%\n",
      "Epoch [85/500] | Loss: 0.0972 | Train: 97.54% | Test: 96.43%\n",
      "Epoch [86/500] | Loss: 0.0961 | Train: 97.60% | Test: 95.73%\n",
      "Epoch [87/500] | Loss: 0.0962 | Train: 97.60% | Test: 96.78%\n",
      "Epoch [88/500] | Loss: 0.0978 | Train: 97.57% | Test: 96.57%\n",
      "Epoch [89/500] | Loss: 0.1029 | Train: 97.30% | Test: 95.52%\n",
      "Epoch [90/500] | Loss: 0.0988 | Train: 97.45% | Test: 96.99%\n",
      "Epoch [91/500] | Loss: 0.0954 | Train: 97.57% | Test: 96.01%\n",
      "Epoch [92/500] | Loss: 0.0923 | Train: 97.60% | Test: 96.64%\n",
      "Epoch [93/500] | Loss: 0.0956 | Train: 97.75% | Test: 96.85%\n",
      "Epoch [94/500] | Loss: 0.0994 | Train: 97.45% | Test: 96.99%\n",
      "Epoch [95/500] | Loss: 0.0975 | Train: 97.60% | Test: 96.78%\n",
      "Epoch [96/500] | Loss: 0.1006 | Train: 97.54% | Test: 97.13%\n",
      "Epoch [97/500] | Loss: 0.0972 | Train: 97.81% | Test: 96.43%\n",
      "Epoch [98/500] | Loss: 0.0956 | Train: 97.81% | Test: 96.57%\n",
      "Epoch [99/500] | Loss: 0.0945 | Train: 97.75% | Test: 96.92%\n",
      "Epoch [100/500] | Loss: 0.0946 | Train: 97.63% | Test: 96.78%\n",
      "Epoch [101/500] | Loss: 0.0936 | Train: 97.72% | Test: 96.57%\n",
      "Epoch [102/500] | Loss: 0.0994 | Train: 97.66% | Test: 96.15%\n",
      "Epoch [103/500] | Loss: 0.0971 | Train: 97.63% | Test: 96.64%\n",
      "Epoch [104/500] | Loss: 0.0987 | Train: 97.72% | Test: 96.78%\n",
      "Epoch [105/500] | Loss: 0.0949 | Train: 97.72% | Test: 96.78%\n",
      "Epoch [106/500] | Loss: 0.0923 | Train: 97.63% | Test: 96.57%\n",
      "Epoch [107/500] | Loss: 0.0967 | Train: 97.57% | Test: 96.08%\n",
      "Epoch [108/500] | Loss: 0.0968 | Train: 97.81% | Test: 96.78%\n",
      "Epoch [109/500] | Loss: 0.0933 | Train: 97.63% | Test: 96.71%\n",
      "Epoch [110/500] | Loss: 0.0951 | Train: 97.60% | Test: 96.29%\n",
      "Epoch [111/500] | Loss: 0.0947 | Train: 97.69% | Test: 96.78%\n",
      "Epoch [112/500] | Loss: 0.0963 | Train: 97.81% | Test: 95.73%\n",
      "Epoch [113/500] | Loss: 0.0922 | Train: 97.90% | Test: 96.22%\n",
      "Epoch [114/500] | Loss: 0.0951 | Train: 97.60% | Test: 96.71%\n",
      "Epoch [115/500] | Loss: 0.0919 | Train: 97.90% | Test: 96.78%\n",
      "Epoch [116/500] | Loss: 0.0951 | Train: 97.39% | Test: 96.78%\n",
      "Epoch [117/500] | Loss: 0.0921 | Train: 97.60% | Test: 96.78%\n",
      "Epoch [118/500] | Loss: 0.0923 | Train: 97.81% | Test: 96.08%\n",
      "Epoch [119/500] | Loss: 0.0954 | Train: 97.45% | Test: 96.78%\n",
      "Epoch [120/500] | Loss: 0.0908 | Train: 97.96% | Test: 96.57%\n",
      "Epoch [121/500] | Loss: 0.0964 | Train: 97.75% | Test: 96.78%\n",
      "Epoch [122/500] | Loss: 0.0983 | Train: 97.51% | Test: 96.78%\n",
      "Epoch [123/500] | Loss: 0.0954 | Train: 97.60% | Test: 81.74%\n",
      "Epoch [124/500] | Loss: 0.0949 | Train: 97.69% | Test: 97.20%\n",
      "Epoch [125/500] | Loss: 0.0930 | Train: 97.84% | Test: 96.78%\n",
      "Epoch [126/500] | Loss: 0.0923 | Train: 97.57% | Test: 96.78%\n",
      "Epoch [127/500] | Loss: 0.0888 | Train: 97.93% | Test: 96.99%\n",
      "Epoch [128/500] | Loss: 0.0956 | Train: 97.60% | Test: 96.85%\n",
      "Epoch [129/500] | Loss: 0.0911 | Train: 97.81% | Test: 96.64%\n",
      "Epoch [130/500] | Loss: 0.0896 | Train: 97.87% | Test: 96.78%\n",
      "Epoch [131/500] | Loss: 0.0944 | Train: 97.60% | Test: 96.71%\n",
      "Epoch [132/500] | Loss: 0.0903 | Train: 97.81% | Test: 96.71%\n",
      "Epoch [133/500] | Loss: 0.0901 | Train: 97.75% | Test: 96.78%\n",
      "Epoch [134/500] | Loss: 0.0886 | Train: 97.96% | Test: 97.20%\n",
      "Epoch [135/500] | Loss: 0.0904 | Train: 97.72% | Test: 96.64%\n",
      "Epoch [136/500] | Loss: 0.0918 | Train: 97.81% | Test: 96.78%\n",
      "Epoch [137/500] | Loss: 0.0948 | Train: 97.66% | Test: 96.64%\n",
      "Epoch [138/500] | Loss: 0.0894 | Train: 97.75% | Test: 93.35%\n",
      "Epoch [139/500] | Loss: 0.0857 | Train: 97.87% | Test: 96.64%\n",
      "Epoch [140/500] | Loss: 0.0940 | Train: 97.60% | Test: 96.78%\n",
      "Epoch [141/500] | Loss: 0.0908 | Train: 97.63% | Test: 96.78%\n",
      "Epoch [142/500] | Loss: 0.0879 | Train: 97.90% | Test: 96.78%\n",
      "Epoch [143/500] | Loss: 0.0872 | Train: 97.84% | Test: 96.01%\n",
      "Epoch [144/500] | Loss: 0.0868 | Train: 97.78% | Test: 96.78%\n",
      "Epoch [145/500] | Loss: 0.0923 | Train: 97.69% | Test: 97.13%\n",
      "Epoch [146/500] | Loss: 0.0854 | Train: 97.93% | Test: 97.06%\n",
      "Epoch [147/500] | Loss: 0.0922 | Train: 97.78% | Test: 97.34%\n",
      "Epoch [148/500] | Loss: 0.0897 | Train: 97.81% | Test: 96.57%\n",
      "Epoch [149/500] | Loss: 0.0880 | Train: 97.75% | Test: 89.29%\n",
      "Epoch [150/500] | Loss: 0.0914 | Train: 97.87% | Test: 96.71%\n",
      "Epoch [151/500] | Loss: 0.0910 | Train: 97.99% | Test: 96.71%\n",
      "Epoch [152/500] | Loss: 0.0874 | Train: 97.84% | Test: 96.85%\n",
      "Epoch [153/500] | Loss: 0.0876 | Train: 97.90% | Test: 96.92%\n",
      "Epoch [154/500] | Loss: 0.0876 | Train: 97.93% | Test: 96.22%\n",
      "Epoch [155/500] | Loss: 0.0851 | Train: 98.05% | Test: 96.43%\n",
      "Epoch [156/500] | Loss: 0.0882 | Train: 97.84% | Test: 96.99%\n",
      "Epoch [157/500] | Loss: 0.0906 | Train: 97.75% | Test: 96.64%\n",
      "Epoch [158/500] | Loss: 0.0952 | Train: 97.54% | Test: 96.64%\n",
      "Epoch [159/500] | Loss: 0.0901 | Train: 97.93% | Test: 96.71%\n",
      "Epoch [160/500] | Loss: 0.0954 | Train: 97.75% | Test: 97.13%\n",
      "Epoch [161/500] | Loss: 0.0869 | Train: 97.87% | Test: 96.43%\n",
      "Epoch [162/500] | Loss: 0.0939 | Train: 97.54% | Test: 96.78%\n",
      "Epoch [163/500] | Loss: 0.0892 | Train: 97.96% | Test: 96.85%\n",
      "Epoch [164/500] | Loss: 0.0880 | Train: 97.75% | Test: 97.20%\n",
      "Epoch [165/500] | Loss: 0.0881 | Train: 97.90% | Test: 96.50%\n",
      "Epoch [166/500] | Loss: 0.0934 | Train: 97.72% | Test: 97.06%\n",
      "Epoch [167/500] | Loss: 0.0901 | Train: 97.78% | Test: 96.85%\n",
      "Epoch [168/500] | Loss: 0.0859 | Train: 97.90% | Test: 97.20%\n",
      "Epoch [169/500] | Loss: 0.0908 | Train: 97.69% | Test: 96.99%\n",
      "Epoch [170/500] | Loss: 0.0885 | Train: 97.96% | Test: 97.06%\n",
      "Epoch [171/500] | Loss: 0.0816 | Train: 98.11% | Test: 96.29%\n",
      "Epoch [172/500] | Loss: 0.0980 | Train: 97.60% | Test: 96.50%\n",
      "Epoch [173/500] | Loss: 0.0943 | Train: 97.63% | Test: 96.85%\n",
      "Epoch [174/500] | Loss: 0.0838 | Train: 97.90% | Test: 96.85%\n",
      "Epoch [175/500] | Loss: 0.0798 | Train: 98.02% | Test: 96.78%\n",
      "Epoch [176/500] | Loss: 0.0845 | Train: 97.75% | Test: 97.13%\n",
      "Epoch [177/500] | Loss: 0.0836 | Train: 97.93% | Test: 96.92%\n",
      "Epoch [178/500] | Loss: 0.0901 | Train: 97.78% | Test: 96.92%\n",
      "Epoch [179/500] | Loss: 0.0845 | Train: 97.87% | Test: 96.71%\n",
      "Epoch [180/500] | Loss: 0.0861 | Train: 97.84% | Test: 97.20%\n",
      "Epoch [181/500] | Loss: 0.0867 | Train: 97.87% | Test: 97.27%\n",
      "Epoch [182/500] | Loss: 0.0883 | Train: 97.84% | Test: 96.50%\n",
      "Epoch [183/500] | Loss: 0.0895 | Train: 97.81% | Test: 96.71%\n",
      "Epoch [184/500] | Loss: 0.0901 | Train: 97.87% | Test: 96.71%\n",
      "Epoch [185/500] | Loss: 0.0874 | Train: 97.84% | Test: 96.99%\n",
      "Epoch [186/500] | Loss: 0.0870 | Train: 97.81% | Test: 96.92%\n",
      "Epoch [187/500] | Loss: 0.0859 | Train: 98.05% | Test: 79.71%\n",
      "Epoch [188/500] | Loss: 0.0844 | Train: 97.96% | Test: 96.78%\n",
      "Epoch [189/500] | Loss: 0.0905 | Train: 97.60% | Test: 10.50%\n",
      "Epoch [190/500] | Loss: 0.0823 | Train: 98.20% | Test: 97.13%\n",
      "Epoch [191/500] | Loss: 0.0871 | Train: 97.78% | Test: 97.06%\n",
      "Epoch [192/500] | Loss: 0.0894 | Train: 97.81% | Test: 92.72%\n",
      "Epoch [193/500] | Loss: 0.0850 | Train: 97.87% | Test: 96.92%\n",
      "Epoch [194/500] | Loss: 0.0825 | Train: 98.05% | Test: 96.36%\n",
      "Epoch [195/500] | Loss: 0.0835 | Train: 97.96% | Test: 96.92%\n",
      "Epoch [196/500] | Loss: 0.0870 | Train: 97.60% | Test: 92.65%\n",
      "Epoch [197/500] | Loss: 0.0852 | Train: 97.84% | Test: 96.85%\n",
      "Epoch [198/500] | Loss: 0.0891 | Train: 97.81% | Test: 96.64%\n",
      "Epoch [199/500] | Loss: 0.0851 | Train: 97.93% | Test: 96.99%\n",
      "Epoch [200/500] | Loss: 0.0853 | Train: 97.87% | Test: 96.85%\n",
      "Epoch [201/500] | Loss: 0.0892 | Train: 97.87% | Test: 96.99%\n",
      "Epoch [202/500] | Loss: 0.0851 | Train: 97.87% | Test: 91.46%\n",
      "Epoch [203/500] | Loss: 0.0900 | Train: 97.87% | Test: 96.71%\n",
      "Epoch [204/500] | Loss: 0.0856 | Train: 97.93% | Test: 96.71%\n",
      "Epoch [205/500] | Loss: 0.0877 | Train: 97.72% | Test: 96.85%\n",
      "Epoch [206/500] | Loss: 0.0856 | Train: 97.69% | Test: 89.92%\n",
      "Epoch [207/500] | Loss: 0.0888 | Train: 97.96% | Test: 35.62%\n",
      "Epoch [208/500] | Loss: 0.0870 | Train: 97.87% | Test: 97.06%\n",
      "Epoch [209/500] | Loss: 0.0874 | Train: 97.75% | Test: 96.78%\n",
      "Epoch [210/500] | Loss: 0.0832 | Train: 97.93% | Test: 96.92%\n",
      "Epoch [211/500] | Loss: 0.0806 | Train: 98.02% | Test: 95.45%\n",
      "Epoch [212/500] | Loss: 0.0839 | Train: 97.90% | Test: 93.42%\n",
      "Epoch [213/500] | Loss: 0.0815 | Train: 98.02% | Test: 79.43%\n",
      "Epoch [214/500] | Loss: 0.0837 | Train: 98.08% | Test: 95.31%\n",
      "Epoch [215/500] | Loss: 0.0828 | Train: 97.93% | Test: 86.98%\n",
      "Epoch [216/500] | Loss: 0.0858 | Train: 97.99% | Test: 90.76%\n",
      "Epoch [217/500] | Loss: 0.0870 | Train: 97.96% | Test: 96.71%\n",
      "Epoch [218/500] | Loss: 0.0871 | Train: 97.81% | Test: 10.50%\n",
      "Epoch [219/500] | Loss: 0.0821 | Train: 97.99% | Test: 88.45%\n",
      "Epoch [220/500] | Loss: 0.0814 | Train: 97.96% | Test: 88.59%\n",
      "Epoch [221/500] | Loss: 0.0815 | Train: 97.96% | Test: 96.99%\n",
      "Epoch [222/500] | Loss: 0.0819 | Train: 98.17% | Test: 96.99%\n",
      "Epoch [223/500] | Loss: 0.0911 | Train: 97.78% | Test: 96.85%\n",
      "Epoch [224/500] | Loss: 0.0837 | Train: 97.99% | Test: 97.13%\n",
      "Epoch [225/500] | Loss: 0.0833 | Train: 98.02% | Test: 96.99%\n",
      "Epoch [226/500] | Loss: 0.0814 | Train: 98.02% | Test: 96.92%\n",
      "Epoch [227/500] | Loss: 0.0824 | Train: 97.81% | Test: 97.06%\n",
      "Epoch [228/500] | Loss: 0.0816 | Train: 97.90% | Test: 96.99%\n",
      "Epoch [229/500] | Loss: 0.0812 | Train: 98.05% | Test: 96.99%\n",
      "Epoch [230/500] | Loss: 0.0869 | Train: 98.05% | Test: 97.13%\n",
      "Epoch [231/500] | Loss: 0.0865 | Train: 97.78% | Test: 96.85%\n",
      "Epoch [232/500] | Loss: 0.0809 | Train: 97.96% | Test: 96.57%\n",
      "Epoch [233/500] | Loss: 0.0800 | Train: 98.08% | Test: 95.59%\n",
      "Epoch [234/500] | Loss: 0.0819 | Train: 98.05% | Test: 94.33%\n",
      "Epoch [235/500] | Loss: 0.0835 | Train: 97.96% | Test: 97.06%\n",
      "Epoch [236/500] | Loss: 0.0847 | Train: 98.05% | Test: 96.71%\n",
      "Epoch [237/500] | Loss: 0.0850 | Train: 97.93% | Test: 94.61%\n",
      "Epoch [238/500] | Loss: 0.0853 | Train: 97.75% | Test: 96.71%\n",
      "Epoch [239/500] | Loss: 0.0831 | Train: 97.96% | Test: 97.13%\n",
      "Epoch [240/500] | Loss: 0.0777 | Train: 98.08% | Test: 87.96%\n",
      "Epoch [241/500] | Loss: 0.0809 | Train: 98.08% | Test: 58.08%\n",
      "Epoch [242/500] | Loss: 0.0803 | Train: 97.96% | Test: 76.21%\n",
      "Epoch [243/500] | Loss: 0.0855 | Train: 97.87% | Test: 96.92%\n",
      "Epoch [244/500] | Loss: 0.0897 | Train: 97.66% | Test: 97.06%\n",
      "Epoch [245/500] | Loss: 0.0800 | Train: 98.05% | Test: 97.27%\n",
      "Epoch [246/500] | Loss: 0.0817 | Train: 97.96% | Test: 85.65%\n",
      "Epoch [247/500] | Loss: 0.0852 | Train: 97.93% | Test: 97.13%\n",
      "Epoch [248/500] | Loss: 0.0848 | Train: 97.99% | Test: 85.65%\n",
      "Epoch [249/500] | Loss: 0.0802 | Train: 97.93% | Test: 77.26%\n",
      "Epoch [250/500] | Loss: 0.0851 | Train: 97.78% | Test: 96.92%\n",
      "Epoch [251/500] | Loss: 0.0811 | Train: 98.02% | Test: 96.71%\n",
      "Epoch [252/500] | Loss: 0.0807 | Train: 97.99% | Test: 91.74%\n",
      "Epoch [253/500] | Loss: 0.0853 | Train: 97.66% | Test: 96.99%\n",
      "Epoch [254/500] | Loss: 0.0843 | Train: 98.14% | Test: 86.00%\n",
      "Epoch [255/500] | Loss: 0.0811 | Train: 98.05% | Test: 30.65%\n",
      "Epoch [256/500] | Loss: 0.0827 | Train: 97.87% | Test: 97.06%\n",
      "Epoch [257/500] | Loss: 0.0785 | Train: 98.05% | Test: 72.29%\n",
      "Epoch [258/500] | Loss: 0.0772 | Train: 98.23% | Test: 93.91%\n",
      "Epoch [259/500] | Loss: 0.0834 | Train: 98.11% | Test: 97.06%\n",
      "Epoch [260/500] | Loss: 0.0851 | Train: 97.90% | Test: 96.64%\n",
      "Epoch [261/500] | Loss: 0.0861 | Train: 97.93% | Test: 97.20%\n",
      "Epoch [262/500] | Loss: 0.0833 | Train: 97.90% | Test: 96.85%\n",
      "Epoch [263/500] | Loss: 0.0844 | Train: 97.87% | Test: 83.76%\n",
      "Epoch [264/500] | Loss: 0.0789 | Train: 97.99% | Test: 96.85%\n",
      "Epoch [265/500] | Loss: 0.0816 | Train: 97.99% | Test: 96.36%\n",
      "Epoch [266/500] | Loss: 0.0821 | Train: 97.99% | Test: 46.47%\n",
      "Epoch [267/500] | Loss: 0.0799 | Train: 98.20% | Test: 95.38%\n",
      "Epoch [268/500] | Loss: 0.0804 | Train: 97.90% | Test: 95.17%\n",
      "Epoch [269/500] | Loss: 0.0781 | Train: 98.11% | Test: 97.06%\n",
      "Epoch [270/500] | Loss: 0.0866 | Train: 97.87% | Test: 97.20%\n",
      "Epoch [271/500] | Loss: 0.0787 | Train: 98.05% | Test: 96.78%\n",
      "Epoch [272/500] | Loss: 0.0831 | Train: 97.84% | Test: 96.71%\n",
      "Epoch [273/500] | Loss: 0.0802 | Train: 98.05% | Test: 96.08%\n",
      "Epoch [274/500] | Loss: 0.0820 | Train: 98.05% | Test: 96.92%\n",
      "Epoch [275/500] | Loss: 0.0809 | Train: 98.02% | Test: 96.92%\n",
      "Epoch [276/500] | Loss: 0.0879 | Train: 97.81% | Test: 78.45%\n",
      "Epoch [277/500] | Loss: 0.0799 | Train: 98.08% | Test: 60.53%\n",
      "Epoch [278/500] | Loss: 0.0801 | Train: 98.14% | Test: 10.50%\n",
      "Epoch [279/500] | Loss: 0.0761 | Train: 98.20% | Test: 96.71%\n",
      "Epoch [280/500] | Loss: 0.0777 | Train: 98.17% | Test: 96.78%\n",
      "Epoch [281/500] | Loss: 0.0810 | Train: 98.11% | Test: 97.20%\n",
      "Epoch [282/500] | Loss: 0.0808 | Train: 97.96% | Test: 91.88%\n",
      "Epoch [283/500] | Loss: 0.0796 | Train: 97.99% | Test: 96.43%\n",
      "Epoch [284/500] | Loss: 0.0772 | Train: 98.14% | Test: 94.82%\n",
      "Epoch [285/500] | Loss: 0.0828 | Train: 98.05% | Test: 96.99%\n",
      "Epoch [286/500] | Loss: 0.0808 | Train: 97.96% | Test: 92.86%\n",
      "Epoch [287/500] | Loss: 0.0795 | Train: 97.99% | Test: 96.57%\n",
      "Epoch [288/500] | Loss: 0.0791 | Train: 98.14% | Test: 48.57%\n",
      "Epoch [289/500] | Loss: 0.0790 | Train: 98.02% | Test: 96.99%\n",
      "Epoch [290/500] | Loss: 0.0751 | Train: 98.11% | Test: 96.64%\n",
      "Epoch [291/500] | Loss: 0.0839 | Train: 97.99% | Test: 96.85%\n",
      "Epoch [292/500] | Loss: 0.0815 | Train: 98.05% | Test: 79.50%\n",
      "Epoch [293/500] | Loss: 0.0745 | Train: 98.11% | Test: 96.99%\n",
      "Epoch [294/500] | Loss: 0.0779 | Train: 98.20% | Test: 96.85%\n",
      "Epoch [295/500] | Loss: 0.0765 | Train: 98.08% | Test: 93.00%\n",
      "Epoch [296/500] | Loss: 0.0797 | Train: 98.05% | Test: 96.64%\n",
      "Epoch [297/500] | Loss: 0.0789 | Train: 98.02% | Test: 97.20%\n",
      "Epoch [298/500] | Loss: 0.0799 | Train: 97.87% | Test: 95.59%\n",
      "Epoch [299/500] | Loss: 0.0764 | Train: 98.08% | Test: 96.85%\n",
      "Epoch [300/500] | Loss: 0.0779 | Train: 98.23% | Test: 96.92%\n",
      "Epoch [301/500] | Loss: 0.0803 | Train: 97.99% | Test: 97.06%\n",
      "Epoch [302/500] | Loss: 0.0809 | Train: 97.93% | Test: 94.82%\n",
      "Epoch [303/500] | Loss: 0.0785 | Train: 98.17% | Test: 94.89%\n",
      "Epoch [304/500] | Loss: 0.0858 | Train: 97.93% | Test: 10.50%\n",
      "Epoch [305/500] | Loss: 0.0820 | Train: 98.02% | Test: 88.17%\n",
      "Epoch [306/500] | Loss: 0.0810 | Train: 97.87% | Test: 96.99%\n",
      "Epoch [307/500] | Loss: 0.0802 | Train: 97.87% | Test: 97.20%\n",
      "Epoch [308/500] | Loss: 0.0775 | Train: 98.14% | Test: 57.17%\n",
      "Epoch [309/500] | Loss: 0.0790 | Train: 97.99% | Test: 97.06%\n",
      "Epoch [310/500] | Loss: 0.0766 | Train: 98.14% | Test: 92.44%\n",
      "Epoch [311/500] | Loss: 0.0783 | Train: 97.96% | Test: 96.78%\n",
      "Epoch [312/500] | Loss: 0.0798 | Train: 97.99% | Test: 94.12%\n",
      "Epoch [313/500] | Loss: 0.0754 | Train: 98.26% | Test: 45.91%\n",
      "Epoch [314/500] | Loss: 0.0817 | Train: 97.81% | Test: 38.77%\n",
      "Epoch [315/500] | Loss: 0.0804 | Train: 98.02% | Test: 96.85%\n",
      "Epoch [316/500] | Loss: 0.0775 | Train: 98.02% | Test: 96.99%\n",
      "Epoch [317/500] | Loss: 0.0803 | Train: 98.08% | Test: 64.24%\n",
      "Epoch [318/500] | Loss: 0.0794 | Train: 97.99% | Test: 97.06%\n",
      "Epoch [319/500] | Loss: 0.0800 | Train: 98.11% | Test: 93.21%\n",
      "Epoch [320/500] | Loss: 0.0765 | Train: 98.02% | Test: 68.23%\n",
      "Epoch [321/500] | Loss: 0.0756 | Train: 98.20% | Test: 96.85%\n",
      "Epoch [322/500] | Loss: 0.0812 | Train: 97.96% | Test: 95.17%\n",
      "Epoch [323/500] | Loss: 0.0800 | Train: 97.99% | Test: 93.42%\n",
      "Epoch [324/500] | Loss: 0.0802 | Train: 98.11% | Test: 95.03%\n",
      "Epoch [325/500] | Loss: 0.0833 | Train: 97.78% | Test: 97.13%\n",
      "Epoch [326/500] | Loss: 0.0763 | Train: 98.14% | Test: 97.06%\n",
      "Epoch [327/500] | Loss: 0.0773 | Train: 98.08% | Test: 97.27%\n",
      "Epoch [328/500] | Loss: 0.0802 | Train: 97.93% | Test: 46.82%\n",
      "Epoch [329/500] | Loss: 0.0754 | Train: 98.08% | Test: 96.29%\n",
      "Epoch [330/500] | Loss: 0.0781 | Train: 97.99% | Test: 91.53%\n",
      "Epoch [331/500] | Loss: 0.0768 | Train: 98.14% | Test: 10.50%\n",
      "Epoch [332/500] | Loss: 0.0759 | Train: 98.11% | Test: 67.46%\n",
      "Epoch [333/500] | Loss: 0.0816 | Train: 98.08% | Test: 21.06%\n",
      "Epoch [334/500] | Loss: 0.0880 | Train: 97.78% | Test: 97.27%\n",
      "Epoch [335/500] | Loss: 0.0749 | Train: 98.08% | Test: 96.78%\n",
      "Epoch [336/500] | Loss: 0.0762 | Train: 98.05% | Test: 10.50%\n",
      "Epoch [337/500] | Loss: 0.0780 | Train: 98.11% | Test: 96.92%\n",
      "Epoch [338/500] | Loss: 0.0806 | Train: 98.11% | Test: 97.20%\n",
      "Epoch [339/500] | Loss: 0.0774 | Train: 98.11% | Test: 62.00%\n",
      "Epoch [340/500] | Loss: 0.0783 | Train: 98.08% | Test: 53.95%\n",
      "Epoch [341/500] | Loss: 0.0779 | Train: 98.14% | Test: 67.74%\n",
      "Epoch [342/500] | Loss: 0.0777 | Train: 98.14% | Test: 59.76%\n",
      "Epoch [343/500] | Loss: 0.0807 | Train: 97.93% | Test: 90.27%\n",
      "Epoch [344/500] | Loss: 0.0801 | Train: 98.11% | Test: 83.69%\n",
      "Epoch [345/500] | Loss: 0.0795 | Train: 97.87% | Test: 80.97%\n",
      "Epoch [346/500] | Loss: 0.0779 | Train: 98.02% | Test: 94.68%\n",
      "Epoch [347/500] | Loss: 0.0779 | Train: 98.02% | Test: 96.29%\n",
      "Epoch [348/500] | Loss: 0.0741 | Train: 98.05% | Test: 95.10%\n",
      "Epoch [349/500] | Loss: 0.0749 | Train: 98.11% | Test: 96.85%\n",
      "Epoch [350/500] | Loss: 0.0780 | Train: 98.05% | Test: 14.07%\n",
      "Epoch [351/500] | Loss: 0.0805 | Train: 97.99% | Test: 80.13%\n",
      "Epoch [352/500] | Loss: 0.0786 | Train: 98.11% | Test: 96.22%\n",
      "Epoch [353/500] | Loss: 0.0813 | Train: 98.02% | Test: 96.99%\n",
      "Epoch [354/500] | Loss: 0.0776 | Train: 98.11% | Test: 90.69%\n",
      "Epoch [355/500] | Loss: 0.0750 | Train: 98.17% | Test: 92.44%\n",
      "Epoch [356/500] | Loss: 0.0784 | Train: 98.02% | Test: 62.42%\n",
      "Epoch [357/500] | Loss: 0.0810 | Train: 98.05% | Test: 97.20%\n",
      "Epoch [358/500] | Loss: 0.0767 | Train: 98.02% | Test: 76.84%\n",
      "Epoch [359/500] | Loss: 0.0753 | Train: 97.93% | Test: 97.20%\n",
      "Epoch [360/500] | Loss: 0.0820 | Train: 97.90% | Test: 94.89%\n",
      "Epoch [361/500] | Loss: 0.0796 | Train: 98.17% | Test: 55.63%\n",
      "Epoch [362/500] | Loss: 0.0745 | Train: 98.02% | Test: 93.63%\n",
      "Epoch [363/500] | Loss: 0.0764 | Train: 98.05% | Test: 96.99%\n",
      "Epoch [364/500] | Loss: 0.0728 | Train: 98.14% | Test: 10.71%\n",
      "Epoch [365/500] | Loss: 0.0746 | Train: 98.20% | Test: 22.53%\n",
      "Epoch [366/500] | Loss: 0.0729 | Train: 98.23% | Test: 60.60%\n",
      "Epoch [367/500] | Loss: 0.0785 | Train: 97.99% | Test: 96.85%\n",
      "Epoch [368/500] | Loss: 0.0705 | Train: 98.32% | Test: 92.65%\n",
      "Epoch [369/500] | Loss: 0.0771 | Train: 97.90% | Test: 92.86%\n",
      "Epoch [370/500] | Loss: 0.0805 | Train: 97.96% | Test: 91.46%\n",
      "Epoch [371/500] | Loss: 0.0785 | Train: 97.90% | Test: 95.38%\n",
      "Epoch [372/500] | Loss: 0.0723 | Train: 98.29% | Test: 96.57%\n",
      "Epoch [373/500] | Loss: 0.0744 | Train: 98.17% | Test: 91.95%\n",
      "Epoch [374/500] | Loss: 0.0745 | Train: 98.17% | Test: 96.57%\n",
      "Epoch [375/500] | Loss: 0.0780 | Train: 98.05% | Test: 81.39%\n",
      "Epoch [376/500] | Loss: 0.0762 | Train: 98.17% | Test: 94.26%\n",
      "Epoch [377/500] | Loss: 0.0740 | Train: 98.20% | Test: 90.27%\n",
      "Epoch [378/500] | Loss: 0.0764 | Train: 98.14% | Test: 9.73%\n",
      "Epoch [379/500] | Loss: 0.0760 | Train: 97.96% | Test: 95.59%\n",
      "Epoch [380/500] | Loss: 0.0776 | Train: 98.11% | Test: 95.87%\n",
      "Epoch [381/500] | Loss: 0.0768 | Train: 98.05% | Test: 95.24%\n",
      "Epoch [382/500] | Loss: 0.0768 | Train: 98.11% | Test: 96.22%\n",
      "Epoch [383/500] | Loss: 0.0824 | Train: 97.84% | Test: 94.26%\n",
      "Epoch [384/500] | Loss: 0.0778 | Train: 97.96% | Test: 67.81%\n",
      "Epoch [385/500] | Loss: 0.0758 | Train: 98.17% | Test: 29.11%\n",
      "Epoch [386/500] | Loss: 0.0740 | Train: 98.05% | Test: 94.82%\n",
      "Epoch [387/500] | Loss: 0.0792 | Train: 98.17% | Test: 9.87%\n",
      "Epoch [388/500] | Loss: 0.0768 | Train: 98.11% | Test: 84.11%\n",
      "Epoch [389/500] | Loss: 0.0747 | Train: 98.14% | Test: 66.48%\n",
      "Epoch [390/500] | Loss: 0.0782 | Train: 98.17% | Test: 96.43%\n",
      "Epoch [391/500] | Loss: 0.0773 | Train: 97.99% | Test: 96.15%\n",
      "Epoch [392/500] | Loss: 0.0777 | Train: 97.99% | Test: 97.20%\n",
      "Epoch [393/500] | Loss: 0.0755 | Train: 98.17% | Test: 97.06%\n",
      "Epoch [394/500] | Loss: 0.0788 | Train: 98.20% | Test: 89.71%\n",
      "Epoch [395/500] | Loss: 0.0724 | Train: 98.14% | Test: 95.03%\n",
      "Epoch [396/500] | Loss: 0.0743 | Train: 97.93% | Test: 96.85%\n",
      "Epoch [397/500] | Loss: 0.0708 | Train: 98.17% | Test: 14.21%\n",
      "Epoch [398/500] | Loss: 0.0761 | Train: 98.08% | Test: 82.86%\n",
      "Epoch [399/500] | Loss: 0.0759 | Train: 98.08% | Test: 82.09%\n",
      "Epoch [400/500] | Loss: 0.0739 | Train: 98.17% | Test: 97.06%\n",
      "Epoch [401/500] | Loss: 0.0789 | Train: 97.90% | Test: 96.85%\n",
      "Epoch [402/500] | Loss: 0.0709 | Train: 98.23% | Test: 95.38%\n",
      "Epoch [403/500] | Loss: 0.0736 | Train: 98.14% | Test: 80.20%\n",
      "Epoch [404/500] | Loss: 0.0750 | Train: 98.02% | Test: 95.24%\n",
      "Epoch [405/500] | Loss: 0.0779 | Train: 98.05% | Test: 39.96%\n",
      "Epoch [406/500] | Loss: 0.0761 | Train: 98.23% | Test: 92.58%\n",
      "Epoch [407/500] | Loss: 0.0739 | Train: 98.05% | Test: 97.06%\n",
      "Epoch [408/500] | Loss: 0.0760 | Train: 98.08% | Test: 90.06%\n",
      "Epoch [409/500] | Loss: 0.0739 | Train: 98.26% | Test: 97.06%\n",
      "Epoch [410/500] | Loss: 0.0767 | Train: 98.08% | Test: 95.73%\n",
      "Epoch [411/500] | Loss: 0.0729 | Train: 98.41% | Test: 92.51%\n",
      "Epoch [412/500] | Loss: 0.0802 | Train: 97.84% | Test: 94.05%\n",
      "Epoch [413/500] | Loss: 0.0743 | Train: 97.90% | Test: 95.66%\n",
      "Epoch [414/500] | Loss: 0.0723 | Train: 98.08% | Test: 97.20%\n",
      "Epoch [415/500] | Loss: 0.0782 | Train: 97.96% | Test: 96.57%\n",
      "Epoch [416/500] | Loss: 0.0739 | Train: 98.14% | Test: 96.92%\n",
      "Epoch [417/500] | Loss: 0.0772 | Train: 97.99% | Test: 97.13%\n",
      "Epoch [418/500] | Loss: 0.0724 | Train: 98.23% | Test: 96.22%\n",
      "Epoch [419/500] | Loss: 0.0739 | Train: 98.14% | Test: 96.78%\n",
      "Epoch [420/500] | Loss: 0.0808 | Train: 97.99% | Test: 95.38%\n",
      "Epoch [421/500] | Loss: 0.0761 | Train: 98.17% | Test: 95.31%\n",
      "Epoch [422/500] | Loss: 0.0731 | Train: 98.20% | Test: 97.06%\n",
      "Epoch [423/500] | Loss: 0.0740 | Train: 98.08% | Test: 95.31%\n",
      "Epoch [424/500] | Loss: 0.0685 | Train: 98.26% | Test: 94.96%\n",
      "Epoch [425/500] | Loss: 0.0740 | Train: 98.17% | Test: 96.85%\n",
      "Epoch [426/500] | Loss: 0.0780 | Train: 98.11% | Test: 81.95%\n",
      "Epoch [427/500] | Loss: 0.0708 | Train: 98.26% | Test: 96.85%\n",
      "Epoch [428/500] | Loss: 0.0812 | Train: 97.93% | Test: 97.13%\n",
      "Epoch [429/500] | Loss: 0.0803 | Train: 97.93% | Test: 95.87%\n",
      "Epoch [430/500] | Loss: 0.0694 | Train: 98.17% | Test: 97.20%\n",
      "Epoch [431/500] | Loss: 0.0791 | Train: 98.14% | Test: 97.20%\n",
      "Epoch [432/500] | Loss: 0.0761 | Train: 98.11% | Test: 97.13%\n",
      "Epoch [433/500] | Loss: 0.0769 | Train: 98.20% | Test: 97.34%\n",
      "Epoch [434/500] | Loss: 0.0770 | Train: 98.20% | Test: 95.73%\n",
      "Epoch [435/500] | Loss: 0.0723 | Train: 98.17% | Test: 93.49%\n",
      "Epoch [436/500] | Loss: 0.0763 | Train: 98.02% | Test: 84.53%\n",
      "Epoch [437/500] | Loss: 0.0709 | Train: 98.20% | Test: 97.13%\n",
      "Epoch [438/500] | Loss: 0.0749 | Train: 98.14% | Test: 88.03%\n",
      "Epoch [439/500] | Loss: 0.0784 | Train: 98.05% | Test: 94.96%\n",
      "Epoch [440/500] | Loss: 0.0835 | Train: 97.78% | Test: 96.29%\n",
      "Epoch [441/500] | Loss: 0.0724 | Train: 98.20% | Test: 96.71%\n",
      "Epoch [442/500] | Loss: 0.0717 | Train: 98.17% | Test: 97.27%\n",
      "Epoch [443/500] | Loss: 0.0744 | Train: 98.08% | Test: 97.27%\n",
      "Epoch [444/500] | Loss: 0.0749 | Train: 98.08% | Test: 95.17%\n",
      "Epoch [445/500] | Loss: 0.0745 | Train: 97.87% | Test: 97.20%\n",
      "Epoch [446/500] | Loss: 0.0745 | Train: 98.23% | Test: 90.13%\n",
      "Epoch [447/500] | Loss: 0.0706 | Train: 98.23% | Test: 95.10%\n",
      "Epoch [448/500] | Loss: 0.0719 | Train: 98.11% | Test: 94.89%\n",
      "Epoch [449/500] | Loss: 0.0723 | Train: 98.17% | Test: 94.96%\n",
      "Epoch [450/500] | Loss: 0.0750 | Train: 97.96% | Test: 95.87%\n",
      "Epoch [451/500] | Loss: 0.0740 | Train: 98.02% | Test: 94.68%\n",
      "Epoch [452/500] | Loss: 0.0715 | Train: 98.20% | Test: 94.89%\n",
      "Epoch [453/500] | Loss: 0.0679 | Train: 98.26% | Test: 96.85%\n",
      "Epoch [454/500] | Loss: 0.0744 | Train: 98.23% | Test: 97.13%\n",
      "Epoch [455/500] | Loss: 0.0722 | Train: 98.14% | Test: 97.06%\n",
      "Epoch [456/500] | Loss: 0.0764 | Train: 98.23% | Test: 10.43%\n",
      "Epoch [457/500] | Loss: 0.0694 | Train: 98.38% | Test: 92.02%\n",
      "Epoch [458/500] | Loss: 0.0802 | Train: 98.20% | Test: 94.89%\n",
      "Epoch [459/500] | Loss: 0.0806 | Train: 98.02% | Test: 96.08%\n",
      "Epoch [460/500] | Loss: 0.0774 | Train: 98.02% | Test: 29.60%\n",
      "Epoch [461/500] | Loss: 0.0737 | Train: 98.11% | Test: 95.03%\n",
      "Epoch [462/500] | Loss: 0.0743 | Train: 98.26% | Test: 96.99%\n",
      "Epoch [463/500] | Loss: 0.0724 | Train: 98.32% | Test: 94.89%\n",
      "Epoch [464/500] | Loss: 0.0719 | Train: 98.14% | Test: 97.13%\n",
      "Epoch [465/500] | Loss: 0.0730 | Train: 98.23% | Test: 49.41%\n",
      "Epoch [466/500] | Loss: 0.0698 | Train: 98.23% | Test: 93.98%\n",
      "Epoch [467/500] | Loss: 0.0728 | Train: 98.32% | Test: 96.43%\n",
      "Epoch [468/500] | Loss: 0.0717 | Train: 98.05% | Test: 94.96%\n",
      "Epoch [469/500] | Loss: 0.0739 | Train: 98.20% | Test: 97.20%\n",
      "Epoch [470/500] | Loss: 0.0710 | Train: 98.23% | Test: 95.59%\n",
      "Epoch [471/500] | Loss: 0.0662 | Train: 98.35% | Test: 79.15%\n",
      "Epoch [472/500] | Loss: 0.0755 | Train: 98.14% | Test: 89.57%\n",
      "Epoch [473/500] | Loss: 0.0725 | Train: 98.20% | Test: 96.85%\n",
      "Epoch [474/500] | Loss: 0.0749 | Train: 98.23% | Test: 97.06%\n",
      "Epoch [475/500] | Loss: 0.0680 | Train: 98.29% | Test: 95.52%\n",
      "Epoch [476/500] | Loss: 0.0752 | Train: 98.14% | Test: 96.15%\n",
      "Epoch [477/500] | Loss: 0.0716 | Train: 98.26% | Test: 94.96%\n",
      "Epoch [478/500] | Loss: 0.0727 | Train: 98.14% | Test: 96.64%\n",
      "Epoch [479/500] | Loss: 0.0739 | Train: 98.14% | Test: 93.56%\n",
      "Epoch [480/500] | Loss: 0.0740 | Train: 98.14% | Test: 94.82%\n",
      "Epoch [481/500] | Loss: 0.0745 | Train: 98.05% | Test: 96.29%\n",
      "Epoch [482/500] | Loss: 0.0707 | Train: 98.23% | Test: 14.00%\n",
      "Epoch [483/500] | Loss: 0.0768 | Train: 98.02% | Test: 93.14%\n",
      "Epoch [484/500] | Loss: 0.0720 | Train: 98.20% | Test: 95.17%\n",
      "Epoch [485/500] | Loss: 0.0746 | Train: 98.11% | Test: 95.94%\n",
      "Epoch [486/500] | Loss: 0.0760 | Train: 97.96% | Test: 55.49%\n",
      "Epoch [487/500] | Loss: 0.0708 | Train: 98.20% | Test: 95.24%\n",
      "Epoch [488/500] | Loss: 0.0735 | Train: 98.20% | Test: 97.06%\n",
      "Epoch [489/500] | Loss: 0.0657 | Train: 98.29% | Test: 95.03%\n",
      "Epoch [490/500] | Loss: 0.0744 | Train: 97.99% | Test: 95.03%\n",
      "Epoch [491/500] | Loss: 0.0736 | Train: 98.14% | Test: 94.75%\n",
      "Epoch [492/500] | Loss: 0.0708 | Train: 98.26% | Test: 84.32%\n",
      "Epoch [493/500] | Loss: 0.0714 | Train: 98.17% | Test: 97.20%\n",
      "Epoch [494/500] | Loss: 0.0744 | Train: 97.96% | Test: 94.75%\n",
      "Epoch [495/500] | Loss: 0.0745 | Train: 98.11% | Test: 97.06%\n",
      "Epoch [496/500] | Loss: 0.0732 | Train: 97.87% | Test: 96.85%\n",
      "Epoch [497/500] | Loss: 0.0683 | Train: 98.23% | Test: 97.13%\n",
      "Epoch [498/500] | Loss: 0.0704 | Train: 98.32% | Test: 48.08%\n",
      "Epoch [499/500] | Loss: 0.0807 | Train: 97.93% | Test: 92.58%\n",
      "Epoch [500/500] | Loss: 0.0724 | Train: 98.17% | Test: 71.24%\n",
      "\n",
      ">>> 训练日志已保存至: log_cnn_koopman.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据集定义 (处理变长数据)\n",
    "# ==========================================\n",
    "class KoopmanDataset(Dataset):\n",
    "    def __init__(self, data_list, labels):\n",
    "        \"\"\"\n",
    "        data_list: list of np.array, 每个元素是一个序列\n",
    "        labels: list of int, 对应的标签\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 将数据转为 Tensor (Float32)\n",
    "        sequence = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sequence, label\n",
    "\n",
    "def load_data_from_txt(file_path):\n",
    "    \"\"\"从txt读取数据，并进行训练集/测试集划分\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"正在加载数据: {file_path} ...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 2: continue # 跳过空行\n",
    "            \n",
    "            # 最后一位是标签\n",
    "            label = int(parts[-1])\n",
    "            # 前面的是序列数据\n",
    "            seq_data = [float(x) for x in parts[:-1]]\n",
    "            \n",
    "            # 过滤掉过短的序列（可选，防止报错）\n",
    "            if len(seq_data) > 5: \n",
    "                sequences.append(np.array(seq_data))\n",
    "                labels.append(label)\n",
    "    \n",
    "    print(f\"总样本数: {len(sequences)}\")\n",
    "    return sequences, labels\n",
    "\n",
    "# --- 关键：Collate Function (用于自动填充/Padding) ---\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    DataLoader 会调用这个函数来打包一个 Batch。\n",
    "    我们需要在这里做 Padding，把不同长度的序列补齐到该 Batch 中最长序列的长度。\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # 1. 获取每个序列的原始长度（RNN可能需要用）\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # 2. padding_value=0 (用0填充)\n",
    "    # batch_first=True -> 输出形状 (Batch_Size, Max_Len)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 3. 增加特征维度 (Batch_Size, Max_Len) -> (Batch_Size, Max_Len, 1)\n",
    "    # 因为 CNN/RNN 通常认为输入是 (Batch, Len, Channels) 或 (Batch, Channels, Len)\n",
    "    padded_seqs = padded_seqs.unsqueeze(-1) \n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return padded_seqs, labels, lengths\n",
    "\n",
    "# ==========================================\n",
    "# 2. 模型定义 (CNN 和 LSTM)\n",
    "# ==========================================\n",
    "\n",
    "# --- 选项 A: 1D-CNN (推荐首选，速度快) ---\n",
    "class KoopmanCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=2):\n",
    "        super(KoopmanCNN, self).__init__()\n",
    "        \n",
    "        # 输入形状: (Batch, Channel=1, Length)\n",
    "        # 注意：在 forward 里我们需要把数据转置一下\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # 全局平均池化，不管长度多少，最后都变成1\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x shape: (Batch, Length, 1)\n",
    "        # Conv1d 需要: (Batch, Channel, Length)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out) # -> (Batch, 64, 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # -> (Batch, 64)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# --- 选项 B: LSTM (适合强时序依赖) ---\n",
    "class KoopmanLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, num_classes=2):\n",
    "        super(KoopmanLSTM, self).__init__()\n",
    "        \n",
    "        # batch_first=True -> 输入 (Batch, Length, Input_Size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x shape: (Batch, Length, 1)\n",
    "        \n",
    "        # 这里的 output 是所有时间步的输出，(h_n, c_n) 是最后时刻的状态\n",
    "        # 如果使用 pack_padded_sequence 效果更好，但为了简单这里直接跑\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # 取最后一个时间步的输出作为分类特征\n",
    "        # 或者取 h_n[-1]\n",
    "        final_feature = h_n[-1] \n",
    "        \n",
    "        out = self.fc(final_feature)\n",
    "        return out\n",
    "\n",
    "# ==========================================\n",
    "# 3. 训练流程\n",
    "# ==========================================\n",
    "\n",
    "def train_model():\n",
    "    # ... (参数设置保持不变) ...\n",
    "    DATA_PATH = r'./data/data_koopman_sequence.txt'\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 500\n",
    "    LEARNING_RATE = 1e-4\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 1. 读取与划分数据\n",
    "    sequences, labels = load_data_from_txt(DATA_PATH)\n",
    "    if not sequences: return\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.3, random_state=42)\n",
    "    train_loader = DataLoader(KoopmanDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(KoopmanDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 2. 初始化模型\n",
    "    model = KoopmanCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 【新增 1】 初始化日志字典\n",
    "    # ==========================================\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    log_data = {\n",
    "        'name': 'CNN (Koopman Seq)',  # 模型名称\n",
    "        'params': total_params,\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    print(f\"模型参数量: {total_params:,}\")\n",
    "    # ==========================================\n",
    "\n",
    "    print(\"开始训练 KoopmanCNN...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        correct = 0; total = 0; running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets, lens in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, lens)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        epoch_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_correct = 0; test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, lens in test_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs, lens)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += targets.size(0)\n",
    "                test_correct += (predicted == targets).sum().item()\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {avg_loss:.4f} | Train: {epoch_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
    "\n",
    "        # ==========================================\n",
    "        # 【新增 2】 记录数据\n",
    "        # ==========================================\n",
    "        log_data['loss'].append(avg_loss)\n",
    "        log_data['train_acc'].append(epoch_acc)\n",
    "        log_data['test_acc'].append(test_acc)\n",
    "        log_data['epochs'].append(epoch + 1)\n",
    "        # ==========================================\n",
    "\n",
    "    # ==========================================\n",
    "    # 【新增 3】 保存为 .npy 文件\n",
    "    # ==========================================\n",
    "    save_path = 'log_cnn_koopman.npy'\n",
    "    np.save(save_path, log_data)\n",
    "    print(f\"\\n>>> 训练日志已保存至: {save_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9123ec1-0e33-4886-bd38-57189fa7e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Part 1] Starting Training Pipeline...\n",
      "Loading data from: ./data/data_koopman_sequence.txt ...\n",
      "Loaded 4763 samples.\n",
      "Model: KoopmanCNN | Total Params: 9,250\n",
      "Training started...\n",
      "Epoch [1/300] Loss: 0.6844 Train Acc: 53.15%\n",
      "Epoch [10/300] Loss: 0.1412 Train Acc: 96.37%\n",
      "Epoch [20/300] Loss: 0.1298 Train Acc: 96.64%\n",
      "Epoch [30/300] Loss: 0.1088 Train Acc: 97.36%\n",
      "Epoch [40/300] Loss: 0.1103 Train Acc: 97.18%\n",
      "Epoch [50/300] Loss: 0.1026 Train Acc: 97.57%\n",
      "Epoch [60/300] Loss: 0.0959 Train Acc: 97.60%\n",
      "Epoch [70/300] Loss: 0.1055 Train Acc: 97.18%\n",
      "Epoch [80/300] Loss: 0.1018 Train Acc: 97.45%\n",
      "Epoch [90/300] Loss: 0.0918 Train Acc: 97.78%\n",
      "Epoch [100/300] Loss: 0.0892 Train Acc: 97.78%\n",
      "Epoch [110/300] Loss: 0.0934 Train Acc: 97.48%\n",
      "Epoch [120/300] Loss: 0.0905 Train Acc: 97.84%\n",
      "Epoch [130/300] Loss: 0.0885 Train Acc: 97.84%\n",
      "Epoch [140/300] Loss: 0.0832 Train Acc: 97.93%\n",
      "Epoch [150/300] Loss: 0.0881 Train Acc: 97.81%\n",
      "Epoch [160/300] Loss: 0.0864 Train Acc: 97.96%\n",
      "Epoch [170/300] Loss: 0.0886 Train Acc: 97.78%\n",
      "Epoch [180/300] Loss: 0.0792 Train Acc: 98.11%\n",
      "Epoch [190/300] Loss: 0.0814 Train Acc: 97.90%\n",
      "Epoch [200/300] Loss: 0.0827 Train Acc: 97.99%\n",
      "Epoch [210/300] Loss: 0.0840 Train Acc: 97.81%\n",
      "Epoch [220/300] Loss: 0.0822 Train Acc: 98.05%\n",
      "Epoch [230/300] Loss: 0.0818 Train Acc: 97.99%\n",
      "Epoch [240/300] Loss: 0.0785 Train Acc: 98.14%\n",
      "Epoch [250/300] Loss: 0.0796 Train Acc: 98.02%\n",
      "Epoch [260/300] Loss: 0.0847 Train Acc: 97.90%\n",
      "Epoch [270/300] Loss: 0.0822 Train Acc: 98.02%\n",
      "Epoch [280/300] Loss: 0.0773 Train Acc: 98.08%\n",
      "Epoch [290/300] Loss: 0.0748 Train Acc: 98.23%\n",
      "Epoch [300/300] Loss: 0.0728 Train Acc: 98.17%\n",
      ">>> Training Finished.\n",
      "\n",
      ">>> [Part 2] Starting Feature Visualization...\n",
      "   -> Processing Input Space features...\n",
      "   -> Extracting CNN Intermediate & Final features...\n",
      "   -> Running t-SNE and plotting...\n",
      ">>> Figure saved to Koopman_CNN_Feature_Evolution.pdf\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # 服务器端绘图\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ================= 配置 =================\n",
    "CONFIG = {\n",
    "    'data_path': r'./data/data_koopman_sequence.txt',\n",
    "    'batch_size': 16,\n",
    "    'epochs': 300,\n",
    "    'lr': 1e-4,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    'save_fig_path': 'Koopman_CNN_Feature_Evolution.pdf' # 保存文件名\n",
    "}\n",
    "# =======================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. 基础组件 (Dataset, Collate, Model)\n",
    "# ==============================================================================\n",
    "\n",
    "class KoopmanDataset(Dataset):\n",
    "    def __init__(self, data_list, labels):\n",
    "        self.data = data_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 转换为 Float32 Tensor\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "def load_data_from_txt(file_path):\n",
    "    \"\"\"从txt读取变长序列数据\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    print(f\"Loading data from: {file_path} ...\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) < 2: continue\n",
    "                label = int(parts[-1])\n",
    "                seq_data = [float(x) for x in parts[:-1]]\n",
    "                if len(seq_data) > 5: \n",
    "                    sequences.append(np.array(seq_data))\n",
    "                    labels.append(label)\n",
    "        print(f\"Loaded {len(sequences)} samples.\")\n",
    "        return sequences, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    padded_seqs = padded_seqs.unsqueeze(-1) # (Batch, Len, 1)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels, lengths\n",
    "\n",
    "class KoopmanCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=2):\n",
    "        super(KoopmanCNN, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Layer 2 (中间特征提取点)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Layer 3 (最终特征提取点)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1) # (Batch, 64, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x = x.permute(0, 2, 1) # (Batch, 1, Len)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    # 【新增】专门用于可视化提取特征的方法\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"返回：中间层特征，最终层特征\"\"\"\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass Layer 1\n",
    "        out1 = self.layer1(x)\n",
    "        \n",
    "        # Pass Layer 2 -> Extract Intermediate\n",
    "        out2 = self.layer2(out1)\n",
    "        # Global Avg Pooling 用于中间层可视化 (Batch, 32)\n",
    "        feat_inter = torch.mean(out2, dim=2) \n",
    "        \n",
    "        # Pass Layer 3 -> Extract Final Latent\n",
    "        out3 = self.layer3(out2) # (Batch, 64, 1)\n",
    "        feat_final = out3.view(out3.size(0), -1) # (Batch, 64)\n",
    "        \n",
    "        return feat_inter, feat_final\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: 训练流程 (Training Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_pipeline():\n",
    "    print(\"\\n>>> [Part 1] Starting Training Pipeline...\")\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    sequences, labels = load_data_from_txt(CONFIG['data_path'])\n",
    "    if not sequences: return None, None\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        sequences, labels, test_size=0.3, random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    train_dataset = KoopmanDataset(X_train, y_train)\n",
    "    test_dataset = KoopmanDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 2. 初始化模型\n",
    "    model = KoopmanCNN().to(CONFIG['device'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    \n",
    "    # 统计参数\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {model.__class__.__name__} | Total Params: {total_params:,}\")\n",
    "    \n",
    "    # 3. 训练循环\n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets, _ in train_loader:\n",
    "            inputs, targets = inputs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # 简单打印进度\n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{CONFIG['epochs']}] Loss: {running_loss/len(train_loader):.4f} Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    print(\">>> Training Finished.\")\n",
    "    \n",
    "    # 打包返回：模型和测试数据（用于可视化）\n",
    "    vis_data = {\n",
    "        'X_test_seq': X_test, # 原始 list (用于 Input Space 可视化)\n",
    "        'y_test': np.array(y_test),\n",
    "        'test_loader': test_loader # 用于提取模型特征\n",
    "    }\n",
    "    return model, vis_data\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: 可视化流程 (Visualization Pipeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def visualize_pipeline(model, vis_data):\n",
    "    print(\"\\n>>> [Part 2] Starting Feature Visualization...\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_test = vis_data['y_test']\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 1. 提取特征 (Feature Extraction)\n",
    "    # -------------------------------------------------------\n",
    "    \n",
    "    # (a) Input Space (Koopman Sequences)\n",
    "    # 需要将变长序列 Padding 成矩阵，以便 t-SNE 处理\n",
    "    print(\"   -> Processing Input Space features...\")\n",
    "    X_test_seq = vis_data['X_test_seq']\n",
    "    \n",
    "    # 简单降采样 (防止序列太长 t-SNE 跑不动)\n",
    "    downsample_rate = 1  # 如果序列很长(>1000)，建议设为 10 或 20\n",
    "    if len(X_test_seq[0]) > 500: downsample_rate = 10\n",
    "    \n",
    "    seqs_ds = [s[::downsample_rate] for s in X_test_seq]\n",
    "    max_len = max(len(s) for s in seqs_ds)\n",
    "    \n",
    "    # Padding 成矩阵 (N, Max_Len)\n",
    "    X_input_mat = np.zeros((len(seqs_ds), max_len))\n",
    "    for i, s in enumerate(seqs_ds):\n",
    "        X_input_mat[i, :len(s)] = s\n",
    "        \n",
    "    # (b) & (c) Model Features\n",
    "    print(\"   -> Extracting CNN Intermediate & Final features...\")\n",
    "    loader = vis_data['test_loader']\n",
    "    feats_inter_list = []\n",
    "    feats_final_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _ in loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            # 调用 extract_features\n",
    "            f_inter, f_final = model.extract_features(inputs)\n",
    "            \n",
    "            feats_inter_list.append(f_inter.cpu().numpy())\n",
    "            feats_final_list.append(f_final.cpu().numpy())\n",
    "            \n",
    "    X_inter_mat = np.concatenate(feats_inter_list, axis=0)\n",
    "    X_final_mat = np.concatenate(feats_final_list, axis=0)\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 2. 降维与绘图 (t-SNE & Plotting)\n",
    "    # -------------------------------------------------------\n",
    "    # 定义绘图数据\n",
    "    data_map = [\n",
    "        ('Koopman Input Space\\n(Raw Sequences)', X_input_mat),\n",
    "        ('CNN Intermediate Features\\n(Layer 2 Output)', X_inter_mat),\n",
    "        ('CNN Final Latent Space\\n(Layer 3 Output)', X_final_mat)\n",
    "    ]\n",
    "    \n",
    "    # 设置风格 (保持与量子图一致)\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\"],\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"figure.titlesize\": 16\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    colors = ['#1f77b4', '#ff7f0e'] # Blue (Normal), Orange (Disruption)\n",
    "    class_names = ['Normal', 'Disruption']\n",
    "    \n",
    "    print(\"   -> Running t-SNE and plotting...\")\n",
    "    \n",
    "    for i, (title, data) in enumerate(data_map):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # t-SNE 配置 (关键参数与量子可视化保持一致)\n",
    "        perp = min(50, len(data)-1) # Perplexity 50 让聚类更紧致\n",
    "        tsne = TSNE(\n",
    "            n_components=2, \n",
    "            perplexity=50,          # 建议尝试 50 或 80，消除长条纹，让簇更圆润\n",
    "            early_exaggeration=20,  # 增大此值，强行拉大类间距离，视觉更震撼\n",
    "            learning_rate='auto',   # 自动学习率\n",
    "            init='pca',             # 使用 PCA 初始化，保留全局结构，图更整齐\n",
    "            max_iter=1000,            # 增加迭代次数，确保收敛\n",
    "            random_state=42\n",
    "        )\n",
    "        emb = tsne.fit_transform(data)\n",
    "        \n",
    "        # 计算 S-Score\n",
    "        try:\n",
    "            score = silhouette_score(data, y_test)\n",
    "        except: score = 0\n",
    "        \n",
    "        # 散点图\n",
    "        for lbl_idx, color in enumerate(colors):\n",
    "            mask = (y_test == lbl_idx)\n",
    "            # ax.scatter(emb[mask, 0], emb[mask, 1], c=color, label=class_names[lbl_idx],\n",
    "            #            alpha=0.75, s=30, edgecolors='w', linewidth=0.3)\n",
    "            ax.scatter(\n",
    "                emb[mask, 0], emb[mask, 1], \n",
    "                c=color, \n",
    "                label=class_names[lbl_idx],\n",
    "                alpha=0.75,   # 透明度从 0.6 提高到 0.75，让颜色更实，对比度更高\n",
    "                s=30,         # 点的大小从 20 提高到 30，让点更清晰\n",
    "                edgecolors='w', # 加白色描边\n",
    "                linewidth=0.3   # 描边细一点\n",
    "            )     \n",
    "            \n",
    "        ax.set_title(f\"({chr(97+i)}) {title}\", fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # S-Score 指标框\n",
    "        ax.text(0.05, 0.92, f'S-Score: {score:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round'))\n",
    "\n",
    "    # 全局图例\n",
    "    handles, _ = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, class_names, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0.0), frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15) # 留出底部图例空间\n",
    "    plt.savefig(CONFIG['save_fig_path'], dpi=300)\n",
    "    print(f\">>> Figure saved to {CONFIG['save_fig_path']}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # 1. 运行训练\n",
    "    trained_model, vis_data = train_pipeline()\n",
    "    \n",
    "    # 2. 运行可视化 (如果训练成功)\n",
    "    if trained_model is not None:\n",
    "        visualize_pipeline(trained_model, vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f01ec5-55d3-4dca-9a12-50c3b9bc0f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
